@article{Dunne2022,
abstract = {Uncertainty quantification is a formal paradigm of statistical estimation that aims to account for all uncertain- ties inherent in the modelling process of real-world complex systems. The methods are directly applicable to stochastic models in epidemiology, however they have thus far not been widely used in this context. In this paper, we provide a tutorial on uncertainty quantification of stochastic epidemic models, aiming to facilitate the use of the uncertainty quantification paradigm for practitioners with other complex stochastic simulators of applied systems. We provide a formal workflow including the important decisions and considerations that need to be taken, and illustrate the methods over a simple stochastic epidemic model of UK SARS-CoV-2 transmission and patient outcome. We also present new approaches to visualisation of outputs from sensitivity analyses and uncertainty quantification more generally in high input and/or output dimensions.},
author = {Dunne, Michael and Mohammadi, Hossein and Challenor, Peter and Borgo, Rita and Porphyre, Thibaud and Vernon, Ian and Firat, Elif E and Turkay, Cagatay and Torsney-weir, Thomas and Goldstein, Michael and Reeve, Richard and Fang, Hui and Swallow, Ben},
doi = {10.1016/j.epidem.2022.100574},
file = {:C\:/Users/md624/OneDrive - University of Exeter/SCRC/Paper/Complex model calibration through emulation, a worked example - published.pdf:pdf},
issn = {1755-4365},
journal = {Epidemics},
keywords = {Calibration,History matching,SEIR,Uncertainty quantification,stochastic epidemic model},
number = {April},
pages = {100574},
publisher = {Elsevier B.V.},
title = {{Complex model calibration through emulation , a worked example for a stochastic epidemic model}},
url = {https://doi.org/10.1016/j.epidem.2022.100574},
volume = {39},
year = {2022}
}

@article{Vernon2014,
abstract = {Cosmologists at the Institute of Computational Cosmology, Durham University, have developed a state of the art model of galaxy formation known as Galform, intended to contribute to our understanding of the formation, growth and subsequent evolution of galaxies in the presence of dark matter. Galform requires the specification of many input parameters and takes a significant time to complete one simulation, making comparison between the model's output and real observations of the Universe extremely challenging. This paper concerns the analysis of this problem using Bayesian emulation within an iterative history matching strategy, and represents the most detailed uncertainty analysis of a galaxy formation simulation yet performed. {\textcopyright} Institute of Mathematical Statistics, 2014.},
annote = {Using Bayesian Inference modelling. 17 inputs. 20 hours per evaluation. Reduced input space down to 0.039% in 5 runs.},
author = {Vernon, Ian and Goldstein, Michael and Bower, Richard},
doi = {10.1214/12-STS412},
file = {:C\:/Users/md624/OneDrive - University of Exeter/Research Papers/43288453.pdf:pdf},
issn = {08834237},
journal = {Statistical Science},
keywords = {Bayes linear,Bayesian statistics,Computer models,Emulation,Galaxy formation,History matching},
number = {1},
pages = {81--90},
title = {{Galaxy formation: Bayesian history matching for the observable universe}},
volume = {29},
year = {2014}
}

@article{Ackland2021,
author = {Ackland, Graeme J and Ackland, James A and Antonioletti, Mario and Wallace, David J},
file = {:C\:/Users/md624/OneDrive - University of Exeter/Research Papers/rsta.2021.0301.pdf:pdf},
journal = {medRxiv},
keywords = {applied mathematics,computational biology,computer modelling and simulation,statistics},
pages = {1--25},
title = {{Fitting the Reproduction number from UK coronavirus case data , and why it is close to 1}},
year = {2021}
}

@article{Williamson2017,
abstract = {In this paper we discuss climate model tuning and present an iterative automatic tuning method from the statistical science literature. The method, which we refer to here as iterative refocussing (though also known as history matching), avoids many of the common pitfalls of automatic tuning procedures that are based on optimisation of a cost function, principally the over-tuning of a climate model due to using only partial observations. This avoidance comes by seeking to rule out parameter choices that we are confident could not reproduce the observations, rather than seeking the model that is closest to them (a procedure that risks over-tuning). We comment on the state of climate model tuning and illustrate our approach through three waves of iterative refocussing of the NEMO (Nucleus for European Modelling of the Ocean) ORCA2 global ocean model run at 2Â° resolution. We show how at certain depths the anomalies of global mean temperature and salinity in a standard configuration of the model exceeds 10 standard deviations away from observations and show the extent to which this can be alleviated by iterative refocussing without compromising model performance spatially. We show how model improvements can be achieved by simultaneously perturbing multiple parameters, and illustrate the potential of using low-resolution ensembles to tune NEMO ORCA configurations at higher resolutions.},
author = {Williamson, Daniel B. and Blaker, Adam T. and Sinha, Bablu},
doi = {10.5194/gmd-10-1789-2017},
file = {:C\:/Users/md624/OneDrive - University of Exeter/Research Papers/gmd-10-1789-2017.pdf:pdf},
issn = {19919603},
journal = {Geoscientific Model Development},
number = {4},
pages = {1789--1816},
title = {{Tuning without over-tuning: Parametric uncertainty quantification for the NEMO ocean model}},
volume = {10},
year = {2017}
}

@article{Lee2011,
abstract = {Sensitivity analysis of atmospheric models is necessary to identify the processes that lead to uncertainty in model predictions, to help understand model diversity through comparison of driving processes, and to prioritise research. Assessing the effect of parameter uncertainty in complex models is challenging and often limited by CPU constraints. Here we present a cost-effective application of variance-based sensitivity analysis to quantify the sensitivity of a 3-D global aerosol model to uncertain parameters. A Gaussian process emulator is used to estimate the model output across multi-dimensional parameter space, using information from a small number of model runs at points chosen using a Latin hypercube space-filling design. Gaussian process emulation is a Bayesian approach that uses information from the model runs along with some prior assumptions about the model behaviour to predict model output everywhere in the uncertainty space. We use the Gaussian process emulator to calculate the percentage of expected output variance explained by uncertainty in global aerosol model parameters and their interactions. To demonstrate the technique, we show examples of cloud condensation nuclei (CCN) sensitivity to 8 model parameters in polluted and remote marine environments as a function of altitude. In the polluted environment 95 % of the variance of CCN concentration is described by uncertainty in the 8 parameters (excluding their interaction effects) and is dominated by the uncertainty in the sulphur emissions, which explains 80 % of the variance. However, in the remote region parameter interaction effects become important, accounting for up to 40 % of the total variance. Some parameters are shown to have a negligible individual effect but a substantial interaction effect. Such sensitivities would not be detected in the commonly used single parameter perturbation experiments, which would therefore underpredict total uncertainty. Gaussian process emulation is shown to be an efficient and useful technique for quantifying parameter sensitivity in complex global atmospheric models. {\textcopyright} 2011 Author(s).},
author = {Lee, L. A. and Carslaw, K. S. and Pringle, K. J. and Mann, G. W. and Spracklen, D. V.},
doi = {10.5194/acp-11-12253-2011},
file = {:C\:/Users/md624/OneDrive - University of Exeter/Research Papers/Emulation_of_a_complex_global_aerosol_model_to_quantify_sensitivity_to_uncertain_parameters.pdf:pdf},
issn = {16807316},
journal = {Atmospheric Chemistry and Physics},
number = {23},
pages = {12253--12273},
title = {{Emulation of a complex global aerosol model to quantify sensitivity to uncertain parameters}},
volume = {11},
year = {2011}
}

@article{Andrianakis2015,
abstract = {Advances in scientific computing have allowed the development of complex models that are being routinely applied to problems in disease epidemiology, public health and decision making. The utility of these models depends in part on how well they can reproduce empirical data. However, fitting such models to real world data is greatly hindered both by large numbers of input and output parameters, and by long run times, such that many modelling studies lack a formal calibration methodology. We present a novel method that has the potential to improve the calibration of complex infectious disease models (hereafter called simulators). We present this in the form of a tutorial and a case study where we history match a dynamic, event-driven, individual-based stochastic HIV simulator, using extensive demographic, behavioural and epidemiological data available from Uganda. The tutorial describes history matching and emulation. History matching is an iterative procedure that reduces the simulator's input space by identifying and discarding areas that are unlikely to provide a good match to the empirical data. History matching relies on the computational efficiency of a Bayesian representation of the simulator, known as an emulator. Emulators mimic the simulator's behaviour, but are often several orders of magnitude faster to evaluate. In the case study, we use a 22 input simulator, fitting its 18 outputs simultaneously. After 9 iterations of history matching, a non-implausible region of the simulator input space was identified that was (Formula presented.) times smaller than the original input space. Simulator evaluations made within this region were found to have a 65% probability of fitting all 18 outputs. History matching and emulation are useful additions to the toolbox of infectious disease modellers. Further research is required to explicitly address the stochastic nature of the simulator as well as to account for correlations between outputs.},
author = {Andrianakis, Ioannis and Vernon, Ian R. and McCreesh, Nicky and McKinley, Trevelyan J. and Oakley, Jeremy E. and Nsubuga, Rebecca N. and Goldstein, Michael and White, Richard G.},
doi = {10.1371/journal.pcbi.1003968},
file = {:C\:/Users/md624/OneDrive - University of Exeter/Research Papers/pcbi.1003968.pdf:pdf},
issn = {15537358},
journal = {PLoS Computational Biology},
number = {1},
pmid = {25569850},
title = {{Bayesian History Matching of Complex Infectious Disease Models Using Emulation: A Tutorial and a Case Study on HIV in Uganda}},
volume = {11},
year = {2015}
}
@article{Vernon2018,
abstract = {Background: Many mathematical models have now been employed across every area of systems biology. These models increasingly involve large numbers of unknown parameters, have complex structure which can result in substantial evaluation time relative to the needs of the analysis, and need to be compared to observed data of various forms. The correct analysis of such models usually requires a global parameter search, over a high dimensional parameter space, that incorporates and respects the most important sources of uncertainty. This can be an extremely difficult task, but it is essential for any meaningful inference or prediction to be made about any biological system. It hence represents a fundamental challenge for the whole of systems biology. Methods: Bayesian statistical methodology for the uncertainty analysis of complex models is introduced, which is designed to address the high dimensional global parameter search problem. Bayesian emulators that mimic the systems biology model but which are extremely fast to evaluate are embeded within an iterative history match: an efficient method to search high dimensional spaces within a more formal statistical setting, while incorporating major sources of uncertainty. Results: The approach is demonstrated via application to a model of hormonal crosstalk in Arabidopsis root development, which has 32 rate parameters, for which we identify the sets of rate parameter values that lead to acceptable matches between model output and observed trend data. The multiple insights into the model's structure that this analysis provides are discussed. The methodology is applied to a second related model, and the biological consequences of the resulting comparison, including the evaluation of gene functions, are described. Conclusions: Bayesian uncertainty analysis for complex models using both emulators and history matching is shown to be a powerful technique that can greatly aid the study of a large class of systems biology models. It both provides insight into model behaviour and identifies the sets of rate parameters of interest.},
archivePrefix = {arXiv},
arxivId = {1607.06358},
author = {Vernon, Ian and Liu, Junli and Goldstein, Michael and Rowe, James and Topping, Jen and Lindsey, Keith},
doi = {10.1186/s12918-017-0484-3},
eprint = {1607.06358},
file = {:C\:/Users/md624/OneDrive - University of Exeter/Research Papers/s12918-017-0484-3.pdf:pdf},
issn = {17520509},
journal = {BMC Systems Biology},
keywords = {Arabidopsis,Bayesian uncertainty analysis,Emulation,Hormonal signalling,Kinetic models,Parameter search,Root development},
number = {1},
pages = {1--29},
pmid = {29291750},
publisher = {BMC Systems Biology},
title = {{Bayesian uncertainty analysis for complex systems biology models: Emulation, global parameter searches and evaluation of gene functions}},
volume = {12},
year = {2018}
}
@article{Currin1991,
abstract = {This article is concerned with prediction of a function y(t) over a (multidimensional) domain T, given the function values at a set of âsitesâ (t(1), t(2), {\ldots}, t(n)) in T, and with the design, that is, with the selection of those sites. The motivating application is the design and analysis of computer experiments, where t determines the input to a computer model of a physical or behavioral system, and y(t) is a response that is part of the output or is calculated from it. Following a Bayesian formulation, prior uncertainty about the function y is expressed by means of a random function Y, which is taken here to be a Gaussian stochastic process. The mean of the posterior process can be used as the prediction function Å·(t), and the variance can be used as a measure of uncertainty. This kind of approach has been used previously in Bayesian interpolation and is strongly related to the kriging methods used in geostatistics. Here emphasis is placed on product linear and product cubic correlation functions, which yield prediction functions that are, respectively, linear or cubic splines in every dimension. A posterior entropy criterion is adopted for design; this minimizes the expected uncertainty about the posterior process, as measured by the entropy. A computational algorithm for finding entropy-optimal designs on multidimensional grids is described. Several examples are presented, including a two-dimensional experiment on a computer model of a thermal energy storage device and a six-dimensional experiment on an integrated circuit simulator. Predictions are made using several different families of correlation functions, with parameters chosen to maximize the likelihood. For comparison, predictions are also made via least squares fitting of various polynomial and spline models. The Bayesian design/prediction methods, which do not require any modeling of y, produce comparatively good predictions. For some correlation functions, however, the 95% posterior probability intervals do not give adequate coverage of the true values of y at selected test sites. These methods are fairly simple and offer considerable potential for virtually automatic implementation, although further development is needed before they can be applied routinely in practice. {\textcopyright} 1991 Taylor & Francis Group, LLC.},
author = {Currin, Carla and Mitchell, Toby and Morris, Max and Ylvisaker, Don},
doi = {10.1080/01621459.1991.10475138},
file = {:C\:/Users/md624/OneDrive - University of Exeter/Research Papers/Bayesian Prediction of Deterministic Functions  with Applications to the Design and Analysis of Computer Experiments.pdf:pdf},
issn = {1537274X},
journal = {Journal of the American Statistical Association},
keywords = {Computer models,Correlation function,Cross-validation,Entropy,Experimental design,Interpolation,Kriging,Optimal design,Spline fitting,Stochastic processes},
number = {416},
pages = {953--963},
title = {{Bayesian prediction of deterministic functions, with applications to the design and analysis of computer experiments}},
volume = {86},
year = {1991}
}
@article{OHagan2006,
abstract = {The Bayesian approach to quantifying, analysing and reducing uncertainty in the application of complex process models is attracting increasing attention amongst users of such models. The range and power of the Bayesian methods is growing and there is already a sizeable literature on these methods. However, most of it is in specialist statistical journals. The purpose of this tutorial is to introduce the more general reader to the Bayesian approach. {\textcopyright} 2005 Elsevier Ltd. All rights reserved.},
annote = {Epistemic uncertainty: uncertainty in non-repeatable events that are due simply to lack of knowledge of those events.},
author = {O'Hagan, Anthony},
doi = {10.1016/j.ress.2005.11.025},
file = {:C\:/Users/md624/OneDrive - University of Exeter/Research Papers/Bayesian analysis of computer code outputs\; A tutorial - A.O'Hagan.pdf:pdf},
issn = {09518320},
journal = {Reliability Engineering and System Safety},
keywords = {Bayesian statistics,Calibration,Dimensionality reduction,Emulator,Gaussian process,Roughness,Screening,Sensitivity analysis,Smoothness,Uncertainty analysis,Validation},
number = {10-11},
pages = {1290--1300},
title = {{Bayesian analysis of computer code outputs: A tutorial}},
volume = {91},
year = {2006}
}
@article{Kennedy2001,
abstract = {We consider prediction and uncertainty analysis for systems which are approximated using complex mathematical models. Such models, implemented as computer codes, are often generic in the sense that by a suitable choice of some of the model's input parameters the code can be used to predict the behaviour of the system in a variety of specific applications. However, in any specific application the values of necessary parameters may be unknown. In this case, physical observations of the system in the specific context are used to learn about the unknown parameters. The process of fitting the model to the observed data by adjusting the parameters is known as calibration. Calibration is typically effected by ad hoc fitting, and after calibration the model is used, with the fitted input values, to predict the future behaviour of the system. We present a Bayesian calibration technique which improves on this traditional approach in two respects. First, the predictions allow for all sources of uncertainty, including the remaining uncertainty over the fitted parameters. Second, they attempt to correct for any inadequacy of the model which is revealed by a discrepancy between the observed data and the model predictions from even the best-fitting parameter values. The method is illustrated by using data from a nuclear radiation release at Tomsk, and from a more complex simulated nuclear accident exercise.},
author = {Kennedy, Marc C. and O'Hagan, Anthony},
doi = {10.1111/1467-9868.00294},
file = {:C\:/Users/md624/OneDrive - University of Exeter/Research Papers/1467-9868.00294.pdf:pdf},
issn = {13697412},
journal = {Journal of the Royal Statistical Society. Series B: Statistical Methodology},
keywords = {Calibration,Computer experiments,Deterministic models,Gaussian process,Interpolation,Model inadequacy,Sensitivity analysis,Uncertainty analysis},
number = {3},
pages = {425--464},
title = {{Bayesian calibration of computer models}},
volume = {63},
year = {2001}
}
@article{Oakley2002,
abstract = {We consider a problem of inference for the output of a computationally expensive computer model. We suppose that the model is to be used in a context where the values of one or more inputs are uncertain, so that the input configuration is a random variable. We require to make inference about the induced distribution of the output. This distri- bution is called the uncertainty distribution, and the general problem is known to users of computer models as uncertainty analysis. To be specific, we develop Bayesian inference for the distribution and density functions of the model output. Modelling the output, as a function of its inputs, as a Gaussian process, we derive expressions for the posterior mean and variance of the distribution and density functions, based on data comprising observed outputs at a sample of input configurations. We show that direct computation of these expressions may encounter numerical difficulties. We develop an alternative approach based on simulating approximate realisations from the posterior distribution of the output function. Two examples are given to illustrate our methods.},
author = {Oakley, Jeremy E. and O'Hagan, Anthony},
doi = {10.1093/biomet/89.4.769},
file = {:C\:/Users/md624/OneDrive - University of Exeter/Research Papers/Bayesian inference for the uncertainty distribution of computer model outputs - J.Oakley, A.O'Hagan.pdf:pdf},
journal = {Oxford University Press on behalf of Biometrika Trust},
keywords = {Computer experiment,Gaussian process,Uncertainty analysis},
mendeley-tags = {Computer experiment,Gaussian process,Uncertainty analysis},
number = {4},
pages = {769--784},
title = {{Bayesian inference for the uncertainty distribution of computer model outputs}},
volume = {89},
year = {2002}
}
@article{Oakley2002_prior,
abstract = {We consider the problem of eliciting expert knowledge about the output of a deterministic computer code, where the output is a function of a vector of input variables. A Gaussian process prior is assumed for the unknown function, and expert judgments about the output at various inputs are used to find suitable hyperparameters of the Gaussian process prior distribution. An example is presented involving the movement of radionuclides in the food chain.},
author = {Oakley, Jeremy},
doi = {10.1111/1467-9884.00300},
file = {:C\:/Users/md624/OneDrive - University of Exeter/Research Papers/J Royal Stat Soc D - 2002 - Oakley - Eliciting Gaussian process priors for complex computer codes.pdf:pdf},
issn = {00390526},
journal = {Journal of the Royal Statistical Society Series D: The Statistician},
keywords = {Computer experiment,Deterministic computer model,Expert elicitation,Gaussian process},
number = {1},
pages = {81--97},
title = {{Eliciting Gaussian process priors for complex computer codes}},
volume = {51},
year = {2002}
}
@book{Rasmussen_Williams_2006,
place={Cambridge, Mass, MA},
title={Gaussian processes for machine learning},
publisher={MIT Press},
author={Rasmussen, Carl Edward and Williams, Christopher K I},
year={2006}
}
@article{Oakley2004,
abstract = {In many areas of science and technology, mathematical models are built to simulate complex real world phenomena. Such models are typically implemented in large computer programs and are also very complex, such that the way that the model responds to changes in its inputs is not transparent. Sensitivity analysis is concerned with understanding how changes in the model inputs influence the outputs. This may be motivated simply by a wish to understand the implications of a complex model but often arises because there is uncertainty about the true values of the inputs that should be used for a particular application. A broad range of measures have been advocated in the literature to quantify and describe the sensitivity of a model's output to variation in its inputs. In practice the most commonly used measures are those that are based on formulating uncertainty in the model inputs by a joint probability distribution and then analysing the induced uncertainty in outputs, an approach which is known as probabilistic sensitivity analysis. We present a Bayesian framework which unifies the various tools of probabilistic sensitivity analysis. The Bayesian approach is computationally highly efficient. It allows effective sensitivity analysis to be achieved by using far smaller numbers of model runs than standard Monte Carlo methods. Furthermore, all measures of interest may be computed from a single set of runs.},
author = {Oakley, Jeremy E. and O'Hagan, Anthony},
doi = {10.1111/j.1467-9868.2004.05304.x},
file = {:C\:/Users/md624/OneDrive - University of Exeter/Research Papers/Probabilistic_sensitivity_analysis_of_computer_models-_a_bayesian_approach.pdf:pdf},
isbn = {1467-9868},
issn = {13697412},
journal = {Journal of the Royal Statistical Society. Series B: Statistical Methodology},
keywords = {Bayesian inference,Computer model,Gaussian process,Sensitivity analysis,Uncertainty analysis},
number = {3},
pages = {751--769},
title = {{Probabilistic sensitivity analysis of complex models: A Bayesian approach}},
volume = {66},
year = {2004}
}
@article{Higdon2008,
abstract = {This work focuses on combining observations from field experiments with detailed computer simulations of a physical process to carry out statistical inference. Of particular interest here is determining uncertainty in resulting predictions. This typically involves calibration of parameters in the computer simulator as well as accounting for inadequate physics in the simulator. The problem is complicated by the fact that simulation code is sufficiently demanding that only a limited number of simulations can be carried out. We consider applications in characterizing material properties for which the field data and the simulator output are highly multivariate. For example, the experimental data and simulation output may be an image or may describe the shape of a physical object. We make use of the basic framework ofKennedy and O'Hagan. However, the size and multivariate nature of the data lead to computational challenges in implementing the framework. To overcome these challenges, we make use of basis representations (e.g., principal components) to reduce the dimensionality of the problem and speed up the computations required for exploring the posterior distribution. This methodology is applied to applications, both ongoing and historical, at Los Alamos National Laboratory},
author = {Higdon, Dave and Gattiker, James and Williams, Brian and Rightley, Maria and Higdon, Dave and Gattiker, James and Williams, Brian and Rightley, Maria and Igdon, Dave H and Attiker, James G and Illiams, Brian W and Ightley, Maria R},
doi = {10.1198/016214507000000888},
file = {:C\:/Users/md624/OneDrive - University of Exeter/Research Papers/Computer Model Calibration Using High Dimensional Output.pdf:pdf},
isbn = {0162145070000},
journal = {American Statistical Association},
keywords = {Uncertainty quantification,computer experiments,functional data analysis,gaussian process,prediction,predictive science},
number = {482},
pages = {570--583},
title = {{Computer Model Calibration Using High- Dimensional Output}},
volume = {103},
year = {2008}
}
@article{Citron2021,
abstract = {Background: Malaria elimination is the goal for Bioko Island, Equatorial Guinea. Intensive interventions implemented since 2004 have reduced prevalence, but progress has stalled in recent years. A challenge for elimination has been malaria infections in residents acquired during travel to mainland Equatorial Guinea. The present article quantifies how off-island contributes to remaining malaria prevalence on Bioko Island, and investigates the potential role of a pre-erythrocytic vaccine in making further progress towards elimination. Methods: Malaria transmission on Bioko Island was simulated using a model calibrated based on data from the Malaria Indicator Surveys (MIS) from 2015 to 2018, including detailed travel histories and malaria positivity by rapid-diagnostic tests (RDTs), as well as geospatial estimates of malaria prevalence. Mosquito population density was adjusted to fit local transmission, conditional on importation rates under current levels of control and within-island mobility. The simulations were then used to evaluate the impact of two pre-erythrocytic vaccine distribution strategies: mass treat and vaccinate, and prophylactic vaccination for off-island travellers. Lastly, a sensitivity analysis was performed through an ensemble of simulations fit to the Bayesian joint posterior probability distribution of the geospatial prevalence estimates. Results: The simulations suggest that in Malabo, an urban city containing 80% of the population, there are some pockets of residual transmission, but a large proportion of infections are acquired off-island by travellers to the mainland. Outside of Malabo, prevalence was mainly attributable to local transmission. The uncertainty in the local transmission vs. importation is lowest within Malabo and highest outside. Using a pre-erythrocytic vaccine to protect travellers would have larger benefits than using the vaccine to protect residents of Bioko Island from local transmission. In simulations, mass treatment and vaccination had short-lived benefits, as malaria prevalence returned to current levels as the vaccine's efficacy waned. Prophylactic vaccination of travellers resulted in longer-lasting reductions in prevalence. These projections were robust to underlying uncertainty in prevalence estimates. Conclusions: The modelled outcomes suggest that the volume of malaria cases imported from the mainland is a partial driver of continued endemic malaria on Bioko Island, and that continued elimination efforts on must account for human travel activity.},
author = {Citron, Daniel T. and Guerra, Carlos A. and Garc{\'{i}}a, Guillermo A. and Wu, Sean L. and Battle, Katherine E. and Gibson, Harry S. and Smith, David L.},
doi = {10.1186/s12936-021-03893-x},
file = {:C\:/Users/md624/OneDrive - University of Exeter/Research Papers/s12936-021-03893-x.pdf:pdf},
issn = {14752875},
journal = {Malaria Journal},
keywords = {Human mobility,Human travel,Malaria connectivity,Malaria importation,Mathematical modelling},
number = {1},
pages = {1--11},
pmid = {34461902},
publisher = {BioMed Central},
title = {{Quantifying malaria acquired during travel and its role in malaria elimination on Bioko Island}},
url = {https://doi.org/10.1186/s12936-021-03893-x},
volume = {20},
year = {2021}
}

@article{Olufsen2000,
abstract = {Blood flow in the large systemic arteries is modeled using one-dimensional equations derived from the axisymmetric Navier-Stokes equations for flow in compliant and tapering vessels. The arterial tree is truncated after the first few generations of large arteries with the remaining small arteries and arterioles providing outflow boundary conditions for the large arteries. By modeling the small arteries and arterioles as a structured tree, a semi-analytical approach based on a linearized version of the governing equations can be used to derive an expression for the root impedance of the structured tree in the frequency domain. In the time domain, this provides the proper outflow boundary condition. The structured tree is a binary asymmetric tree in which the radii of the daughter vessels are scaled linearly with the radius of the parent vessel. Blood flow and pressure in the large vessels are computed as functions of time and axial distance within each of the arteries. Comparison between the simulations and magnetic resonance measurements in the ascending aorta and nine peripheral locations in one individual shows excellent agreement between the two.},
author = {Olufsen, Mette S. and Peskin, Charles S. and Kim, Won Yong and Pedersen, Erik M. and Nadim, Ali and Larsen, Jesper},
doi = {10.1114/1.1326031},
file = {:C\:/Users/md624/OneDrive - University of Exeter/Research Papers/1.1326031.pdf:pdf},
issn = {00906964},
journal = {Annals of Biomedical Engineering},
keywords = {address correspondence to mette,arterial blood flow,arterial modeling,arterial outflow conditions,biofluid dynamics,blood flow,department of math-,mathematical modeling,modeling,olufsen,s},
number = {11},
pages = {1281--1299},
pmid = {11212947},
title = {{Numerical simulation and experimental validation of blood flow in arteries with structured-tree outflow conditions}},
volume = {28},
year = {2000}
}

@article{Vernon2022,
abstract = {We analyze JUNE: a detailed model of COVID-19 transmission with high spatial and demographic resolution, developed as part of the RAMP initiative. JUNE requires substantial computational resources to evaluate, making model calibration and general uncertainty analysis extremely challenging. We describe and employ the uncertainty quantification approaches of Bayes linear emulation and history matching to mimic JUNE and to perform a global parameter search, hence identifying regions of parameter space that produce acceptable matches to observed data, and demonstrating the capability of such methods. This article is part of the theme issue 'Technical challenges of modelling real-life epidemics and examples of overcoming these'.},
author = {Vernon, I and Owen, J and Aylett-Bullock, J. and Cuesta-Lazaro, C. and Frawley, J and Quera-Bofarull, A. and Sedgewick, A and Shi, D and Truong, H and Turner, M and Walker, J and Caulfield, T and Fong, K and Krauss, F},
doi = {10.1098/rsta.2022.0039},
file = {:C\:/Users/md624/OneDrive - University of Exeter/Research Papers/rsta.2022.0039.pdf:pdf},
isbn = {0000000175},
issn = {1364503X},
journal = {Philosophical Transactions of the Royal Society A: Mathematical, Physical and Engineering Sciences},
keywords = {Bayes linear,calibration,disease models,emulation,history matching},
number = {2233},
pmid = {35965471},
title = {{Bayesian emulation and history matching of JUNE}},
volume = {380},
year = {2022}
}

@book{Neal_1996,
place={New York, NY},
title={Bayesian learning for Neural Networks},
publisher={Springer New York},
author={Neal, Radford M.},
year={1996}
}

@article{Andrianakis2012,
abstract = {The effect of a Gaussian process parameter known as the nugget, on the development of computer model emulators is investigated. The presence of the nugget results in an emulator that does not interpolate the data and attaches a non-zero uncertainty bound around them. The limits of this approximation are investigated theoretically, and it is shown that they can be as large as those of a least squares model with the same regression functions as the emulator, regardless of the nugget's value. The likelihood of the correlation function parameters is also studied and two mode types are identified. Type I modes are characterised by an approximation error that is a function of the nugget and can therefore become arbitrarily small, effectively yielding an interpolating emulator. Type II modes result in emulators with a constant approximation error. Apart from a theoretical investigation of the limits of the approximation error, a practical method for automatically imposing restrictions on its extent is introduced. This is achieved by means of a penalty term that is added to the likelihood function, and controls the amount of unexplainable variability in the computer model. The main findings are illustrated on data from an Energy Balance climate model. {\textcopyright} 2012 Elsevier B.V. All rights reserved.},
author = {Andrianakis, Ioannis and Challenor, Peter G.},
doi = {10.1016/j.csda.2012.04.020},
file = {:C\:/Users/md624/OneDrive - University of Exeter/Research Papers/1-s2.0-S0167947312001879-main.pdf:pdf},
issn = {01679473},
journal = {Computational Statistics and Data Analysis},
keywords = {Approximation,Computer experiments,Ill conditioning,Interpolation,Kriging},
number = {12},
pages = {4215--4228},
publisher = {Elsevier B.V.},
title = {{The effect of the nugget on Gaussian process emulators of computer models}},
url = {http://dx.doi.org/10.1016/j.csda.2012.04.020},
volume = {56},
year = {2012}
}

@article{Baker2020,
abstract = {Statistically modeling the output of a stochastic computer model can be difficult to do accurately without a large simulation budget. We alleviate this problem by exploiting readily available deterministic approximations to efficiently learn about the respective stochastic computer models. This is done via the summation of two Gaussian processes; one responsible for modeling the deterministic approximation, the other responsible for using such approximation to better statistically model the stochastic computer model. The developed method provides high predictive performance and increased confidence that complicated features of a stochastic computer model are captured, even when the simulation budget is small. Several synthetic computer models are used to outline the capabilities of this method, and two real-world examples are used to display its practical utility. Supplementary materials for this article are available online.},
archivePrefix = {arXiv},
arxivId = {1902.01290},
author = {Baker, Evan and Challenor, Peter and Eames, Matt},
doi = {10.1080/10618600.2020.1750416},
eprint = {1902.01290},
file = {:C\:/Users/md624/OneDrive - University of Exeter/Research Papers/Predicting the Output From a Stochastic Computer Model When a Deterministic Approximation is Available.pdf:pdf},
issn = {15372715},
journal = {Journal of Computational and Graphical Statistics},
keywords = {Emulation,Gaussian process,Heteroscedastic,Multifidelity,Stochastic kriging,Stochastic simulation},
number = {4},
pages = {786--797},
title = {{Predicting the Output From a Stochastic Computer Model When a Deterministic Approximation is Available}},
volume = {29},
year = {2020}
}

@article{Neal1997,
abstract = {Gaussian processes are a natural way of defining prior distributions over functions of one or more input variables. In a simple nonparametric regression problem, where such a function gives the mean of a Gaussian distribution for an observed response, a Gaussian process model can easily be implemented using matrix computations that are feasible for datasets of up to about a thousand cases. Hyperparameters that define the covariance function of the Gaussian process can be sampled using Markov chain methods. Regression models where the noise has a t distribution and logistic or probit models for classification applications can be implemented by sampling as well for latent values underlying the observations. Software is now available that implements these methods using covariance functions with hierarchical parameterizations. Models defined in this way can discover high-level properties of the data, such as which inputs are relevant to predicting the response.},
archivePrefix = {arXiv},
arxivId = {physics/9701026},
author = {Neal, Radford M},
eprint = {9701026},
file = {:C\:/Users/md624/OneDrive - University of Exeter/Research Papers/9701026.pdf:pdf},
keywords = {Automatic relevance determination},
number = {9702},
pages = {1--24},
primaryClass = {physics},
title = {{Monte Carlo Implementation of Gaussian Process Models for Bayesian Regression and Classification}},
url = {http://arxiv.org/abs/physics/9701026},
year = {1997}
}

@article{Gramacy2012,
abstract = {Most surrogate models for computer experiments are interpolators, and the most common interpolator is a Gaussian process (GP) that deliberately omits a small-scale (measurement) error term called the nugget. The explanation is that computer experiments are, by definition, "deterministic", and so there is no measurement error. We think this is too narrow a focus for a computer experiment and a statistically inefficient way to model them. We show that estimating a (non-zero) nugget can lead to surrogate models with better statistical properties, such as predictive accuracy and coverage, in a variety of common situations. {\textcopyright} 2010 Springer Science+Business Media, LLC.},
archivePrefix = {arXiv},
arxivId = {1007.4580},
author = {Gramacy, Robert B. and Lee, Herbert K.H.},
doi = {10.1007/s11222-010-9224-x},
eprint = {1007.4580},
file = {:C\:/Users/md624/OneDrive - University of Exeter/Research Papers/s11222-010-9224-x.pdf:pdf},
issn = {09603174},
journal = {Statistics and Computing},
keywords = {Computer simulator,Gaussian process,Interpolation,Smoothing,Surrogate model},
number = {3},
pages = {713--722},
title = {{Cases for the nugget in modeling computer experiments}},
volume = {22},
year = {2012}
}

@article{Craig2001,
abstract = {Although computer models are often used for forecasting future outcomes of complex systems, the uncertainties in such forecasts are not usually treated formally. We describe a general Bayesian approach for using a computer model or simulator of a complex system to forecast system outcomes. The approach is based on constructing beliefs derived from a combination of expert judgments and experiments on the computer model. These beliefs, which are systematically updated as we make runs of the computer model, are used for either Bayesian or Bayes linear forecasting for the system. Issues of design and diagnostics are described in the context of forecasting. The methodology is applied to forecasting for an active hydrocarbon reservoir. {\textcopyright} 2001 American Statistical Association.},
author = {Craig, Peter S. and Goldstein, Michael and Rougier, Jonathan C. and Seheult, Allan H.},
doi = {10.1198/016214501753168370},
file = {:C\:/Users/md624/OneDrive - University of Exeter/Research Papers/Bayesian Forecasting for Complex Systems Using Computer Simulators.pdf:pdf},
isbn = {0162145017531},
issn = {1537274X},
journal = {Journal of the American Statistical Association},
keywords = {Bayes linear methods,Calibration,Computer experiments,Design,Diagnostics,History matching,Hydrocarbon reservoir},
number = {454},
pages = {717--729},
title = {{Bayesian forecasting for complex systems using computer simulators}},
volume = {96},
year = {2001}
}

@article{Vernon2010,
abstract = {In many scientific disciplines complex computer models are used to understand the behaviour of large scale physical systems. An uncertainty analysis of such a computer model known as Galform is presented. Galform models the creation and evolution of approximately one million galaxies from the beginning of the Universe until the current day, and is regarded as a state-of-the-art model within the cosmology community. It requires the specification of many in-put parameters in order to run the simulation, takes significant time to run, and provides various outputs that can be compared with real world data. A Bayes Linear approach is presented in order to identify the subset of the input space that could give rise to acceptable matches between model output and measured data. This approach takes account of the major sources of uncertainty in a consistent and unified manner, including input parameter uncertainty, function uncertainty, observational error, forcing function uncertainty and structural uncertainty. The approach is known as History Matching, and involves the use of an iterative succession of emulators (stochastic belief specifications detailing beliefs about the Galform function), which are used to cut down the input parameter space. The analysis was successful in producing a large collection of model evaluations that exhibit good fits to the observed data. {\textcopyright} 2010 International Society for Bayesian Analysis.},
author = {Vernon, Ian and Goldstein, Michael and Bower, Richard},
doi = {10.1214/10-BA524},
file = {:C\:/Users/md624/OneDrive - University of Exeter/Research Papers/10-BA524.pdf:pdf},
issn = {19360975},
journal = {Bayesian Analysis},
keywords = {Bayes linear analysis,Computer models,Galaxy formation,Galform,History matching,Model discrepancy,Uncertainty analysis},
number = {4},
pages = {619--670},
title = {{Galaxy formation: A Bayesian uncertainty analysis}},
volume = {5},
year = {2010}
}

@article{Bower2010,
abstract = {Semi-analytic models are a powerful tool for studying the formation of galaxies. However, these models inevitably involve a significant number of poorly constrained parameters that must be adjusted to provide an acceptable match to the observed Universe. In this paper, we set out to quantify the degree to which observational data sets can constrain the model parameters. By revealing degeneracies in the parameter space we can hope to better understand the key physical processes probed by the data. We use novel mathematical techniques to explore the parameter space of the GALFORM semi-analytic model.We base our investigation on the Bower et al. version of GALFORM, adopting the same methodology of selecting model parameters based on an acceptable match to the local bJ and K luminosity functions. Since the GALFORM model is inherently approximate, we explicitly include a model discrepancy term when deciding if a match is acceptable or not. The model contains 16 parameters that are poorly constrained by our prior understanding of the galaxy formation processes and that can plausibly be adjusted between reasonable limits. We investigate this parameter space using the Model Emulator technique, constructing a Bayesian approximation to the GALFORM model that can be rapidly evaluated at any point in parameter space. The emulator returns both an expectation for the GALFORM model and an uncertainty which allows us to eliminate regions of parameter space in which it is implausible that a GALFORM run would match the luminosity function data. By combining successive waves of emulation, we show that only 0.26 per cent of the initial volume is of interest for further exploration. However, within this region we show that the Bower et al. model is only one choice from an extended subspace of model parameters that can provide equally acceptable fits to the luminosity function data. We explore the geometry of this region and begin to explore the physical connections between parameters that are exposed by this analysis.We also consider the impact of adding additional observational data to further constrain the parameter space. We see that the known tensions existing in the Bower et al. model lead to a further reduction in the successful parameter space. {\textcopyright} 2010 The Authors. Journal compilation. {\textcopyright} 2010 RAS.},
archivePrefix = {arXiv},
arxivId = {1004.0711},
author = {Bower, R. G. and Vernon, I. and Goldstein, M. and Benson, A. J. and Lacey, C. G. and Baugh, C. M. and Cole, S. and Frenk, C. S.},
doi = {10.1111/j.1365-2966.2010.16991.x},
eprint = {1004.0711},
file = {:C\:/Users/md624/OneDrive - University of Exeter/Research Papers/mnras0407-2017.pdf:pdf},
issn = {13652966},
journal = {Monthly Notices of the Royal Astronomical Society},
keywords = {Galaxies: formation,Galaxies: luminosity function, mass function},
number = {4},
pages = {2017--2045},
title = {{The parameter space of galaxy formation}},
volume = {407},
year = {2010}
}

@article{Sacks1989,
abstract = {Many scientific phenomena are now investigated by complex computer models or codes. A computer experiment is a number of runs of the code with various inputs. A feature of many computer experiments is that the output is deterministic-rerunning the code with the same inputs gives identical observations. Often, the codes are computationally expensive to run, and a common objective of an experiment is to fit a cheaper predictor of the output to the data. Our approach is to model the deterministic output as the realization of a stochastic process, thereby providing a statistical basis for designing experiments (choosing the inputs) for efficient predic- tion. With this model, estimates of uncertainty of predictions are also available. Recent work in this area is reviewed, a number of applications are discussed, and we demonstrate our methodology with an example.},
author = {Sacks, Jerome and Welch, William J. and Mitchell, Toby and Wynn, Henry P.},
doi = {10.2514/6.1998-4757},
file = {:C\:/Users/md624/OneDrive - University of Exeter/Research Papers/2245858.pdf:pdf},
journal = {Statistical Science},
keywords = {computer-aided design,experimental design,kriging,response surface,spatial statistics},
number = {4},
pages = {409--435},
title = {{Design and analysis of computer experiments}},
volume = {4},
year = {1989}
}

@article{Andrianakis2011,
author = {Andrianakis, Y and Challenor, P G},
file = {:C\:/Users/md624/OneDrive - University of Exeter/Research Papers/Parameter_estimation_for_Gaussian_process_emulator.pdf:pdf},
journal = {Power},
number = {August},
pages = {1--25},
title = {{Parameter estimation for Gaussian process emulators}},
year = {2011}
}

@article{Linkletter2006,
abstract = {In many situations, simulation of complex phenomena requires a large number of inputs and is computationally expensive. Identifying the inputs that most impact the system so that these factors can be further investigated can be a critical step in the scientific endeavor. In computer experiments, it is common to use a Gaussian spatial process to model the output of the simulator. In this article we introduce a new, simple method for identifying active factors in computer screening experiments. The approach is Bayesian and only requires the generation of a new inert variable in the analysis; however, in the spirit of frequentist hypothesis testing, the posterior distribution of the inert factor is used as a reference distribution against which the importance of the experimental factors can be assessed. The methodology is demonstrated on an application in material science, a computer experiment from the literature, and simulated examples.},
author = {Linkletter, Crystal and Bingham, Derak and Hengartner, Nicholas and Higdon, David and Ye, Kenny Q.},
doi = {10.1198/004017006000000228},
file = {:C\:/Users/md624/OneDrive - University of Exeter/Research Papers/Variable Selection for Gaussian Process Models in Computer Experiments.pdf:pdf},
issn = {00401706},
journal = {Technometrics},
keywords = {Computer simulation,Latin hypercube,Random field,Screening,Spatial process},
number = {4},
pages = {478--490},
title = {{Variable selection for Gaussian process models in computer experiments}},
volume = {48},
year = {2006}
}
@article{Fisher_1925,
title={Book review statistical methods for research workers}, 
volume={193}, 
number={18},
journal={The Boston Medical and Surgical Journal},
author={Fisher, R A},
year={1925},
pages={854â855}
}
@article{Wilk1957,
author = {Wilk, M. B. and Kempthorne, Oscar},
file = {:C\:/Users/md624/OneDrive - University of Exeter/Research Papers/Wilk-NonAdditivesLatinSquare-1957.pdf:pdf},
journal = {Journal of the American Statistical Association},
number = {278},
pages = {218-236},
title = {Non-Additives in a Latin Square Design},
url = {https://www.jstor.org/stable/2280847},
volume = {52},
year = {1957}
}
@article{Fisher_1926,
title={The arrangement of field experiments}, 
volume={503}, 
number={13},
journal={Journal of the Ministry of Agriculture},
author={Fisher, R A},
year={1926}
}
@article{Mckay1979,
abstract = {Two types of sampling plans are examined as alternatives to simple random sampling in Monte Carlo studies. These plans are shown to be improvements over simple random sampling with respect to variance for a class of estimators which includes the sample mean and the empirical distribution function.},
author = {Mckay, Author M D and Beckman, R J and Conover},
doi = {https://doi.org/10.2307%2F1268522},
file = {:C\:/Users/md624/OneDrive - University of Exeter/Research Papers/1268522.pdf:pdf},
journal = {Technometrics},
keywords = {Latin hypercube sampling,Sampling techniques,Simulation techniques,Variance reduction},
number = {2},
pages = {239--245},
title = {{A Comparison of Three Methods for Selecting Values of Input Variables in the Analysis of Output from a Computer Code A Comparison of Three Methods for Selecting Values of Input Variables in the Analysis of Output from a Computer Code}},
volume = {21},
year = {1979}
}
@article{Iman1988,
abstract = {Many different techniques have been proposed for performing uncertainty and sensitivity analyses on computer models for complex processes. The objective of the present study is to investigate the applicability of three widely used techniques to three computer models having large uncertainties and varying degrees of complexity in order to highlight some of the problem areas that must be addressed in actual applications. The following approaches to uncertainty and sensitivity analysis are considered: (1) response surface methodology based on input determined from a fractional factorial design; (2) Latin hypercube sampling with and without regression analysis; and (3) differential analysis. These techniques are investigated with respect to (1) ease of implementation, (2) flexibility, (3) estimation of the cumulative distribution function of the output, and (4) adaptability to different methods of sensitivity analysis. With respect to these criteria, the technique using Latin hypercube sampling and regression analysis had the best overall performance. The models used in the investigation are well documented, thus making it possible for researchers to make comparisons of other techniques with the results in this study. Copyright {\textcopyright} 1988, Wiley Blackwell. All rights reserved},
author = {Iman, Ronald L. and Helton, Jon C.},
doi = {10.1111/j.1539-6924.1988.tb01155.x},
issn = {15396924},
journal = {Risk Analysis},
keywords = {Latin hypercube sampling,Monte Carlo,differential analysis,response surface,risk assessment},
number = {1},
pages = {71-90},
title = {An Investigation of Uncertainty and Sensitivity Analysis Techniques for Computer Models},
volume = {8},
year = {1988}
}
@article{Helton1993,
abstract = {Uncertainty and sensitivity analysis techniques for use in performance assessments for radioactive waste disposal are reviewed. Summaries are given for the following techniques: differential analysis, Monte Carlo analysis, response surface methodology, and Fourier amplitude sensitivity test. Of these techniques, Monte Carlo analysis is felt to be the most widely applicable for use in performance assessment. Monte Carlo analysis involves five steps: (1) selection of a range and distribution for each input variable; (2) generation of a sample from the input variables; (3) propagation of the sample through the model under consideration; (4) performance of uncertainty analysis; and (5) performance of sensitivity analysis. These steps are discussed and illustrated with an analysis performed as part of a preliminary performance assessment for the Waste Isolation Pilot Plant (WIPP). {\textcopyright} 1993.},
author = {Helton, Jon C.},
doi = {10.1016/0951-8320(93)90097-I},
file = {:C\:/Users/md624/OneDrive - University of Exeter/Research Papers/1-s2.0-095183209390097I-main.pdf:pdf},
issn = {09518320},
journal = {Reliability Engineering and System Safety},
number = {2-3},
pages = {327-367},
title = {Uncertainty and sensitivity analysis techniques for use in performance assessment for radioactive waste disposal},
volume = {42},
year = {1993}
}
@article{Sobol1967,
author = {Sobol', I. M.},
file = {:C\:/Users/md624/OneDrive - University of Exeter/Research Papers/0041-55532990144-9.pdf:pdf},
issn = {00444669},
journal = {USSR Computational Mathematics and Mathematical Physics},
number = {4},
pages = {86--112},
title = {{Point distribution in a cube and approximate evaluation of integrals}},
url = {https://doi.org/10.1016/0041-5553(67)90144-9},
volume = {7},
year = {1967}
}
@article{Bratley1988,
abstract = {We compare empirically accuracy and speed of low-discrepancy sequence generators of Sobol' and Faure. These generators are useful for multidimensional integration and global optimization. We discuss our implementation of the Sobol' generator. {\textcopyright} 1988, ACM. All rights reserved.},
author = {Bratley, Paul and Fox, Bennett L.},
doi = {10.1145/42288.214372},
file = {:C\:/Users/md624/OneDrive - University of Exeter/Research Papers/42288.214372.pdf:pdf},
issn = {15577295},
journal = {ACM Transactions on Mathematical Software (TOMS)},
keywords = {computational complexity,discrepancy,generators,global optimization,multidimensional integration,quasirandom sequences},
number = {1},
pages = {88--100},
title = {{Algorithm 659: Implementing Sobol's Quasirandom Sequence Generator}},
volume = {14},
year = {1988}
}
@article{Joe2008,
abstract = {Direction numbers for generating Sobol' sequences that satisfy the so-called Property A in up to 1111 dimensions have previously been given in Joe and Kuo [ACM Trans. Math. Software, 29 (2003), pp. 49-57]. However, these Sobol' sequences may have poor two-dimensional projections. Here we provide a new set of direction numbers alleviating this problem. These are obtained by treating Sobol' sequences in d dimensions as (t, d)-sequences and then optimizing the t-values of the two-dimensional projections. Our target dimension is 21201. {\textcopyright} 2008 Society for Industrial and Applied Mathematics.},
author = {Joe, Stephen and Kuo, Frances Y},
doi = {10.1137/070709359},
file = {:C\:/Users/md624/OneDrive - University of Exeter/Research Papers/070709359.pdf:pdf},
issn = {10648275},
journal = {SIAM Journal on Scientific Computing},
keywords = {Digital nets and sequences,Numerical integration,Quasi-monte carlo methods,Sobol' sequences,Two-dimensional projections},
number = {5},
pages = {2635--2654},
title = {{Constructing Sobol' sequences with better two-dimensional projections}},
volume = {30},
year = {2008}
}
@article{Burhenne2011,
abstract = {Monte Carlo (MC) techniques are commonly used to perform uncertainty and sensitivity analyses. A key element of MC methods is the sampling of input parameters for the simulation, where the goal is to explore the entire input space with a reasonable sample size (N). The sample size determines the computational cost of the analysis since N is equal to the required number of simulation runs. Quasi-random (QR) sequences such as the Sobolâ² sequences are designed to generate a sample that is uniformly distributed over the unit hypercube. In this paper, sampling based on Sobolâ² sequences is compared with other standard sampling procedures with respect to typical building simulation applications. The work revealed that for the most of the analyzed aspects the sampling based on Sobolâ² sequences performs better than the other investigated sampling techniques.},
author = {Burhenne, Sebastian and Jacob, Dirk and Henze, Gregor P.},
file = {:C\:/Users/md624/OneDrive - University of Exeter/Research Papers/BS2011_Burhenne_Sampling.pdf:pdf},
journal = {Proceedings of Building Simulation 2011: 12th Conference of International Building Performance Simulation Association},
number = {May},
pages = {1816--1823},
title = {{Sampling based on sobol' sequences for monte carlo techniques applied to building simulations}},
year = {2011}
}
@article{Stein1987,
abstract = {Latin hypercube sampling (McKay, Conover, and Beckman 1979) is a method of sampling that can be used to produce input values for estimation of expectations of functions of output variables. The asymptotic variance of such an estimate is obtained. The estimate is also shown to be asymptotically normal. Asymptotically, the variance is less than that obtained using simple random sampling, with the degree of variance reduction depending on the degree of additivity in the function being integrated. A method for producing Latin hypercube samples when the components of the input variables are statistically dependent is also described. These techniques are applied to a simulation of the performance of a printer actuator.},
author = {Stein, Michael},
doi = {10.1080/00401706.1987.10488205},
issn = {15372723},
journal = {Technometrics},
keywords = {Exchangeability,Rank procedure,Sampling with dependent random variables,Variance reduction},
number = {2},
pages = {143--151},
title = {{Large sample properties of simulations using latin hypercube sampling}},
volume = {29},
year = {1987}
}
@misc{Goldstein2006,
abstract = {A calibration-based approach is developed tor predicting the behavior of a physical system that is modeled by a computer simulator. The approach is based on Bayes linear adjustment using both system observations and evaluations of the simulator at parameterizations that appear to give good matches to those observations. This approach can be applied to complex high-dimensional systems with expensive simulators, where a fully Bayesian approach would be impractical. It is illustrated with an example concerning the collapse of the thermohaline circulation (THC) in the Atlantic Ocean. {\textcopyright} 2006 American Statistical Association.},
author = {Goldstein, Michael and Rougier, Jonathan},
booktitle = {Journal of the American Statistical Association},
doi = {10.1198/016214506000000203},
file = {:C\:/Users/md624/OneDrive - University of Exeter/Research Papers/Bayes Linear Calibrated Prediction for Complex Systems.pdf:pdf},
issn = {01621459},
keywords = {Calibration,Computer experiment,Emulator,Hat run,Model diagnostics,Simulator,Thermohaline circulation},
number = {475},
pages = {1132--1143},
title = {{Bayes linear calibrated prediction for complex systems}},
volume = {101},
year = {2006}
}
@article{Plischke2013,
abstract = {Simulation models support managers in the solution of complex problems. International agencies recommend uncertainty and global sensitivity methods as best practice in the audit, validation and application of scientific codes. However, numerical complexity, especially in the presence of a high number of factors, induces analysts to employ less informative but numerically cheaper methods. This work introduces a design for estimating global sensitivity indices from given data (including simulation input-output data), at the minimum computational cost. We address the problem starting with a statistic based on the L1-norm. A formal definition of the estimators is provided and corresponding consistency theorems are proved. The determination of confidence intervals through a bias-reducing bootstrap estimator is investigated. The strategy is applied in the identification of the key drivers of uncertainty for the complex computer code developed at the National Aeronautics and Space Administration (NASA) assessing the risk of lunar space missions. We also introduce a symmetry result that enables the estimation of global sensitivity measures to datasets produced outside a conventional input-output functional framework. {\textcopyright} 2012 Elsevier B.V. All rights reserved.},
author = {Plischke, Elmar and Borgonovo, Emanuele and Smith, Curtis L},
doi = {10.1016/j.ejor.2012.11.047},
file = {:C\:/Users/md624/OneDrive - University of Exeter/Research Papers/1-s2.0-S0377221712008995-main.pdf:pdf},
issn = {03772217},
journal = {European Journal of Operational Research},
keywords = {Global sensitivity analysis,Simulation,Uncertainty analysis},
number = {3},
pages = {536--550},
publisher = {Elsevier B.V.},
title = {{Global sensitivity measures from given data}},
url = {http://dx.doi.org/10.1016/j.ejor.2012.11.047},
volume = {226},
year = {2013}
}
@article{Tripathy2016,
abstract = {Uncertainty quantification (UQ) tasks, such as model calibration, uncertainty propagation, and optimization under uncertainty, typically require several thousand evaluations of the underlying computer codes. To cope with the cost of simulations, one replaces the real response surface with a cheap surrogate based, e.g., on polynomial chaos expansions, neural networks, support vector machines, or Gaussian processes (GP). However, the number of simulations required to learn a generic multivariate response grows exponentially as the input dimension increases. This curse of dimensionality can only be addressed, if the response exhibits some special structure that can be discovered and exploited. A wide range of physical responses exhibit a special structure known as an active subspace (AS). An AS is a linear manifold of the stochastic space characterized by maximal response variation. The idea is that one should first identify this low dimensional manifold, project the high-dimensional input onto it, and then link the projection to the output. If the dimensionality of the AS is low enough, then learning the link function is a much easier problem than the original problem of learning a high-dimensional function. The classic approach to discovering the AS requires gradient information, a fact that severely limits its applicability. Furthermore, and partly because of its reliance to gradients, it is not able to handle noisy observations. The latter is an essential trait if one wants to be able to propagate uncertainty through stochastic simulators, e.g., through molecular dynamics codes. In this work, we develop a probabilistic version of AS which is gradient-free and robust to observational noise. Our approach relies on a novel Gaussian process regression with built-in dimensionality reduction. In particular, the AS is represented as an orthogonal projection matrix that serves as yet another covariance function hyper-parameter to be estimated from the data. To train the model, we design a two-step maximum likelihood optimization procedure that ensures the orthogonality of the projection matrix by exploiting recent results on the Stiefel manifold, i.e., the manifold of matrices with orthogonal columns. The additional benefit of our probabilistic formulation, is that it allows us to select the dimensionality of the AS via the Bayesian information criterion. We validate our approach by showing that it can discover the right AS in synthetic examples without gradient information using both noiseless and noisy observations. We demonstrate that our method is able to discover the same AS as the classical approach in a challenging one-hundred-dimensional problem involving an elliptic stochastic partial differential equation with random conductivity. Finally, we use our approach to study the effect of geometric and material uncertainties in the propagation of solitary waves in a one dimensional granular system.},
archivePrefix = {arXiv},
arxivId = {1602.04550},
author = {Tripathy, Rohit and Bilionis, Ilias and Gonzalez, Marcial},
doi = {10.1016/j.jcp.2016.05.039},
eprint = {1602.04550},
file = {:C\:/Users/md624/OneDrive - University of Exeter/Research Papers/j.jcp.2016.05.039.pdf:pdf},
issn = {10902716},
journal = {Journal of Computational Physics},
keywords = {Active subspace,Dimensionality reduction,Gaussian process regression,Granular crystals,Stiefel manifold,Uncertainty quantification},
pages = {191--223},
publisher = {Elsevier Inc.},
title = {{Gaussian processes with built-in dimensionality reduction: Applications to high-dimensional uncertainty propagation}},
url = {http://dx.doi.org/10.1016/j.jcp.2016.05.039},
volume = {321},
year = {2016}
}
@article{Matern1947,
  title={Methods of estimating the accuracy of line and sample plot surveys.},
  author={Mat{\'e}rn, B},
  journal={Swedish with English summary},
  year={1947}
}
@article{Despotovic2020,
abstract = {Parkinson's disease is a progressive neurodegenerative disorder often accompanied by impairment in articulation, phonation, prosody and fluency of speech. In fact, speech impairment is one of the earliest Parkinson's disease symptoms, and may be used for early diagnosis. We present an experimental study of identification of Parkinson's disease and assessment of disease progress from speech using Gaussian processes, which is further combined with Automatic Relevance Determination (ARD) for efficient feature selection. Hyperparameters of ARD covariance functions are learned for each individual feature; therefore, can be used for evaluation of their importance. In that way only a small subset of highly relevant acoustic features is selected, leading to models with better performance and lower complexity. The performance of the proposed method was assessed on two datasets: Parkinson's disease detection dataset, which contains a range of biomedical voice measurements obtained from 31 subjects, 23 of them suffering from Parkinson's disease and 8 healthy subjects; and Parkinson's telemonitoring dataset, containing biomedical voice measurements collected from 42 Parkinson's disease patients for estimation of the disease progress. Gaussian process classification with automatic relevance determination is able to successfully discriminate between Parkinson's disease patients and healthy controls with 96.92% accuracy, outperforming Support Vector Machines and decision tree ensembles (random forests, boosted and bagged decision trees). The usability of Gaussian processes is further confirmed in regression task for tracking the progress of the disease.},
author = {Despotovic, Vladimir and Skovranek, Tomas and Schommer, Christoph},
doi = {10.1016/j.neucom.2020.03.058},
file = {:C\:/Users/md624/OneDrive - University of Exeter/Research Papers/1-s2.0-S0925231220304318-main.pdf:pdf},
issn = {18728286},
journal = {Neurocomputing},
keywords = {Automatic relevance determination,Feature selection,Gaussian processes,Machine learning,Parkinson's disease,Speech disorder},
pages = {173--181},
publisher = {Elsevier B.V.},
title = {{Speech Based Estimation of Parkinson's Disease Using Gaussian Processes and Automatic Relevance Determination}},
url = {https://doi.org/10.1016/j.neucom.2020.03.058},
volume = {401},
year = {2020}
}
@article{Tuo2020,
author = {Tuo, Rui and Wang, Wenjia},
title = {Kriging Prediction with Isotropic Mat\'{e}rn Correlations: Robustness and Experimental Designs},
year = {2020},
issue_date = {January 2020},
publisher = {JMLR.org},
volume = {21},
number = {1},
issn = {1532-4435},
abstract = {This work investigates the prediction performance of the kriging predictors. We derive some error bounds for the prediction error in terms of non-asymptotic probability under the uniform metric and Lp metrics when the spectral densities of both the true and the imposed correlation functions decay algebraically. The Mat\'{e}rn family is a prominent class of correlation functions of this kind. Our analysis shows that, when the smoothness of the imposed correlation function exceeds that of the true correlation function, the prediction error becomes more sensitive to the space-filling property of the design points. In particular, the kriging predictor can still reach the optimal rate of convergence, if the experimental design scheme is quasi-uniform. Lower bounds of the kriging prediction error are also derived under the uniform metric and Lp metrics. An accurate characterization of this error is obtained, when an oversmoothed correlation function and a space-filling design is used.},
journal = {J. Mach. Learn. Res.},
month = {jan},
articleno = {187},
numpages = {38},
keywords = {space-filling designs, scattered data approximation, uncertainty quantification, Bayesian machine learning, computer experiments}
}
@article{Cunningham2008,
author = {Cunningham, John P. and Shenoy, Krishna V. and Sahani, Maneesh},
title = {Fast Gaussian Process Methods for Point Process Intensity Estimation},
year = {2008},
isbn = {9781605582054},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1390156.1390181},
doi = {10.1145/1390156.1390181},
abstract = {Point processes are difficult to analyze because they provide only a sparse and noisy observation of the intensity function driving the process. Gaussian Processes offer an attractive framework within which to infer underlying intensity functions. The result of this inference is a continuous function defined across time that is typically more amenable to analytical efforts. However, a naive implementation will become computationally infeasible in any problem of reasonable size, both in memory and run time requirements. We demonstrate problem specific methods for a class of renewal processes that eliminate the memory burden and reduce the solve time by orders of magnitude.},
booktitle = {Proceedings of the 25th International Conference on Machine Learning},
pages = {192â199},
numpages = {8},
location = {Helsinki, Finland},
series = {ICML '08}
}
@article{Holden2015,
abstract = {Running complex computer models can be expensive in computer time, while learning about the relationships between input and output variables can be difficult. An emulator is a fast approximation to a computationally expensive model that can be used as a surrogate for the model, to quantify uncertainty or to improve process understanding. Here, we examine emulators based on singular value decompositions (SVDs) and use them to emulate global climate and vegetation fields, examining how these fields are affected by changes in the Earth's orbit. The vegetation field may be emulated directly from the orbital variables, but an appealing alternative is to relate it to emulations of the climate fields, which involves high-dimensional input and output. The SVDs radically reduce the dimensionality of the input and output spaces and are shown to clarify the relationships between them. The method could potentially be useful for any complex process with correlated, high-dimensional inputs and/or outputs.},
author = {Holden, Philip B. and Edwards, Neil R. and Garthwaite, Paul H. and Wilkinson, Richard D.},
doi = {10.1080/02664763.2015.1016412},
file = {:C\:/Users/md624/OneDrive - University of Exeter/Research Papers/Emulation and interpretation of high-dimensional climate model outputs.pdf:pdf},
issn = {13600532},
journal = {Journal of Applied Statistics},
keywords = {climate modelling,coupled models,emulation,principal components,singular value decomposition},
number = {9},
pages = {2038--2055},
title = {{Emulation and interpretation of high-dimensional climate model outputs}},
url = {https://doi.org/10.1080/02664763.2015.1016412},
volume = {42},
year = {2015}
}
@article{Liu2017,
abstract = {As a well-known approximation method, Kriging is widely used in process engineering design and opti-mization for saving computational budget. The Kriging model for a target function is fitted to a set ofsample points, the responses of which are expensive to obtain in practice and the sample distribution ofwhich has a great impact on the model prediction quality. Therefore, a main task in adaptive samplingfor Kriging metamodeling is to gather informative points in order to build an accurate model with asfew points as possible. To this end, we propose an adaptive sampling approach under the bias-variancedecomposition framework. This novel sampling approach sequentially selects new points by maximizingan expected prediction error criterion that considers both the bias and variance information. Particularly,it presents an adaptive balance strategy to dynamically balance the local exploitation and global explo-ration via the error information from the previous iteration. Four benchmark cases and four engineeringcases from low to high dimensions are used to assess the performance of the proposed approach. Numeri-cal results reveal that this adaptive sampling approach is very promising for constructing accurate Krigingmodels for problems with diverse characteristics.},
author = {Liu, Haitao and Cai, Jianfei and Ong, Yew-soon},
doi = {10.1016/j.compchemeng.2017.05.025},
file = {:C\:/Users/md624/OneDrive - University of Exeter/Research Papers/1-s2.0-S009813541730234X-main.pdf:pdf},
issn = {0098-1354},
journal = {Computers and Chemical Engineering},
keywords = {Adaptive balance strategy,Adaptive sampling,Expected prediction error,Kriging metamodeling},
mendeley-groups = {UQ Group},
mendeley-tags = {Adaptive balance strategy,Adaptive sampling,Expected prediction error,Kriging metamodeling},
pages = {171--182},
publisher = {Elsevier Ltd},
title = {{An adaptive sampling approach for Kriging metamodeling by maximizing expected prediction error}},
url = {http://dx.doi.org/10.1016/j.compchemeng.2017.05.025},
volume = {106},
year = {2017}
}
@article{Mohammadi2021,
abstract = {A Gaussian process (GP)-based methodology is proposed to emulate complex dynamical computer models (or simulators). The method relies on emulating the short-time numerical flow map of the system, where the flow map is a function that returns the solution of a dynamical system at a certain time point, given initial conditions. Here, the flow map is emulated via a GP whose kernel is approximated with random Fourier features. This yields a random predictor whose realisations are approximations to the flow map. In order to predict a given time series (i.e., the model output), a single realisation of the approximate flow map is taken and used to iterate from the initial condition ahead in time. Repeating this procedure with multiple realisations from the distribution of approximate flow maps creates a distribution over the time series whose mean and variance serve as the model output prediction and the associated uncertainty, respectively. The proposed method is applied to emulate several dynamic nonlinear simulators including the well-known Lorenz and van der Pol models. The results suggest that our approach has a high predictive performance and the associated uncertainty can capture the dynamics of the system accurately. Additionally, our approach has potential for "embarrassingly" parallel implementations where one can conduct the iterative predictions performed by a realisation on a single computing node.},
annote = {RFF: Random Fourier Features
MAE: Mean Absolute Error},
archivePrefix = {arXiv},
arxivId = {2104.14987},
author = {Mohammadi, Hossein and Challenor, Peter and Goodfellow, Marc},
eprint = {2104.14987},
file = {:C\:/Users/md624/OneDrive - University of Exeter/Research Papers/2104.14987v4.pdf:pdf},
keywords = {dynamical simulator,emulation,gaussian process,random},
pages = {1--25},
title = {{Emulating complex dynamical simulators with random Fourier features}},
url = {http://arxiv.org/abs/2104.14987},
year = {2021}
}
@article{Ming2021,
abstract = {We propose a novel deep Gaussian process (DGP) inference method for computer model emulation using stochastic imputation. By stochastically imputing the latent layers, the approach transforms the DGP into the linked GP, a state-of-the-art surrogate model formed by linking a system of feed-forward coupled GPs. This transformation renders a simple while efficient DGP training procedure that only involves optimizations of conventional stationary GPs. In addition, the analytically tractable mean and variance of the linked GP allows one to implement predictions from DGP emulators in a fast and accurate manner. We demonstrate the method in a series of synthetic examples and real-world applications, and show that it is a competitive candidate for efficient DGP surrogate modeling in comparison to the variational inference and the fully-Bayesian approach. A $\texttt{Python}$ package $\texttt{dgpsi}$ implementing the method is also produced and available at https://github.com/mingdeyu/DGP.},
archivePrefix = {arXiv},
arxivId = {2107.01590},
author = {Ming, Deyu and Williamson, Daniel and Guillas, Serge},
eprint = {2107.01590},
file = {:C\:/Users/md624/OneDrive - University of Exeter/Research Papers/2107.01590.pdf:pdf},
keywords = {Elliptical Slice Sampling,Linked Gaussian Processes,Option Greeks,Stochastic Expectation Maximization,Surrogate Model},
mendeley-groups = {UQ Group},
number = {2003},
title = {{Deep Gaussian Process Emulation using Stochastic Imputation}},
url = {http://arxiv.org/abs/2107.01590},
year = {2021}
}
@Manual{R-base,
    title = {R: A Language and Environment for Statistical Computing},
    author = {{R Core Team}},
    organization = {R Foundation for Statistical Computing},
    address = {Vienna, Austria},
    year = {2021},
    url = {https://www.R-project.org/},
}
@Article{DiceKriging,
    title = {{DiceKriging}, {DiceOptim}: Two {R} Packages for the Analysis of Computer Experiments by Kriging-Based Metamodeling and Optimization},
    author = {Olivier Roustant and David Ginsbourger and Yves Deville},
    journal = {Journal of Statistical Software},
    year = {2012},
    volume = {51},
    number = {1},
    pages = {1--55},
    url = {https://www.jstatsoft.org/v51/i01/},
}
@Manual{lhs,
    title = {lhs: Latin Hypercube Samples},
    author = {Rob Carnell},
    year = {2021},
    note = {R package version 1.1.3},
    url = {https://CRAN.R-project.org/package=lhs},
}
@book{Fang2005,
place = {Boca Raton,FL},
title = {Design and modeling for computer experiments},
publisher = {Chapman \& Hall/CRC},
author = {Fang,Kaitai and Li, Run-ze and Sudjianto, Agus},
year = {2005}
}
@book{Santner2003,
address={New York},
title={The Design and analysis of computer experiments},
ISBN={9780387954202},
publisher={Springer},
author={Santner, Thomas J and Williams, Brian J and Notz, William},
year={2003}
}
@book{Krzanowski1988,
address={New York},
title={Principles of Multivariate Analysis},
ISBN={9780198522119},
publisher={Oxford University Press, USA},
author={Krzanowski, W J},
year={1988}
}
@book{OHagan2004,
volume = {2B},
title = {Kendall's Advanced Theory of Statistics, volume 2B: Bayesian Inference, second edition},
author = {Anthony O'Hagan and Jonathan J. Forster},
publisher = {Arnold},
year = {2004},
url = {https://eprints.soton.ac.uk/46376/}
}
@article{Higdon2004,
author = {Higdon, Dave and Kennedy, Marc C. and Cavendish, James C. and Cafeo, J. and Ryne, Robert D.},
doi = {10.1137/S1064827503426693},
file = {:C\:/Users/md624/OneDrive - University of Exeter/Research Papers/s1064827503426693.pdf:pdf},
journal = {SIAM Journal on Scientific Computing},
keywords = {Calibration,Computer experiments,Gaussian process,Model validation,Predictability,Simulator science,Uncertainty quantification},
number = {2},
pages = {448--466},
title = {{Combining Field Data and Computer Simulations for Calibration and Prediction}},
volume = {26},
year = {2004}
}
@article{Metropolis1957,
author = {Metropolis, Nicholas and Rosenbluth, Arianna W. and Rosenbluth, Marshall N. and Teller, Augusta H. and Teller, Edward},
title = "{Equation of State Calculations by Fast Computing Machines}",
journal = {The Journal of Chemical Physics},
volume = {21},
number = {6},
pages = {1087-1092},
year = {1957},
month = {12},
abstract = "{A general method, suitable for fast computing machines, for investigating such properties as equations of state for substances consisting of interacting individual molecules is described. The method consists of a modified Monte Carlo integration over configuration space. Results for the twoâdimensional rigidâsphere system have been obtained on the Los Alamos MANIAC and are presented here. These results are compared to the free volume equation of state and to a fourâterm virial coefficient expansion.}",
issn = {0021-9606},
doi = {10.1063/1.1699114},
url = {https://doi.org/10.1063/1.1699114},
eprint = {https://pubs.aip.org/aip/jcp/article-pdf/21/6/1087/8115285/1087\_1\_online.pdf},
}
@article{Saltelli2000,
abstract = {We explore the tasks where sensitivity analysis (SA) can be useful and try to assess the relevance of SA within the modeling process. We suggest that SA could considerably assist in the use of models, by providing objective criteria of judgement for different phases of the model-building process: model identification and discrimination; model calibration; model corroboration. We review some new global quantitative SA methods and suggest that these might enlarge the scope for sensitivity analysis in computational and statistical modeling practice. Among the advantages of the new methods are their robustness, model independence and computational convenience. The discussion is based on worked examples. {\textcopyright} 2000 Institute of Mathematical Statistics.},
author = {Saltelli, A and Tarantola, S and Campolongo, F},
doi = {10.1214/ss/1009213004},
file = {:C\:/Users/md624/OneDrive - University of Exeter/Research Papers/2676831.pdf:pdf},
issn = {08834237},
journal = {Statistical Science},
keywords = {Global sensitivity analysis,Model transparency,Numerical experiments,Predictive uncertainty,Quantitative sensitivity measure,Reliability and dependability of models,Screening},
number = {4},
pages = {377--395},
title = {{Sensitivity analysis as an ingredient of modeling}},
volume = {15},
year = {2000}
}
@article{Craig1997,
abstract = {In the oil industry, fluid flow models for reservoirs are usually too complex to be solved analytically and approximate numerical solutions must be obtained using a âreservoir simulator', a complex computer program which takes as input descriptions of the reservoir geology. We describe a Bayes linear strategy for history matching; that is, seeking simulator inputs for which the outputs match closely to historical production. This approach, which only requires specification of means, variances and covariances, formally combines reservoir engineers' beliefs with data from fast approximations to the simulator. We present an account of our experiences in applying the strategy to match the pressure history of an active reservoir. The methodology is appropriate in a wide variety of applications involving inverse problems in computer experiments.},
author = {Craig, Peter S. and Goldstein, Michael and Seheult, Allan H. and Smith, James A.},
doi = {10.1007/978-1-4612-2290-3_2},
file = {:C\:/Users/md624/OneDrive - University of Exeter/Research Papers/craig1997.pdf:pdf},
keywords = {history matching},
pages = {37--93},
title = {{Pressure Matching for Hydrocarbon Reservoirs: A Case Study in the Use of Bayes Linear Strategies for Large Computer Experiments}},
year = {1997}
}
@phdthesis{Salter2017,
abstract = {In this thesis, we present novel methodology for emulating and calibrating computer models with high-dimensional output. Computer models for complex physical systems, such as climate, are typically expensive and time-consuming to run. Due to this inability to run computer models efficiently, statistical models (âemulators') are used as fast approximations of the computer model, fitted based on a small number of runs of the expensive model, allowing more of the input parameter space to be explored. Common choices for emulators are regressions and Gaussian processes. The input parameters of the computer model that lead to output most consistent with the observations of the real-world system are generally unknown, hence computer models require careful tuning. Bayesian calibration and history matching are two methods that can be combined with emulators to search for the best input parameter setting of the computer model (calibration), or remove regions of parameter space unlikely to give output consistent with the observations, if the computer model were to be run at these settings (history matching). When calibrating computer models, it has been argued that fitting regression emulators is sufficient, due to the large, sparsely-sampled input space. We examine this for a range of examples with different features and input dimensions, and find that fitting a correlated residual term in the emulator is beneficial, in terms of more accurately removing regions of the input space, and identifying parameter settings that give output consistent with the observations. We demonstrate and advocate for multi-wave history matching followed by calibration for tuning. In order to emulate computer models with large spatial output, projection onto a low-dimensional basis is commonly used. The standard accepted method for selecting a basis is to use n runs of the computer model to compute principal components via the singular 3 value decomposition (the SVD basis), with the coefficients given by this projection emulated. We show that when the n runs used to define the basis do not contain important patterns found in the real-world observations of the spatial field, linear combinations of the SVD basis vectors will not generally be able to represent these observations. Therefore, the results of a calibration exercise are meaningless, as we converge to incorrect parameter settings, likely assigning zero posterior probability to the correct region of input space. We show that the inadequacy of the SVD basis is very common and present in every climate model field we looked at. We develop a method for combining important patterns from the observations with signal from the model runs, developing a calibration-optimal rotation of the SVD basis that allows a search of the output space for fields consistent with the observations. We illustrate this method by performing two iterations of history matching on a climate model, CanAM4. We develop a method for beginning to assess model discrepancy for climate models, where modellers would first like to see whether the model can achieve certain accuracy, before allowing specific model structural errors to be accounted for. We show that calibrating using the basis coefficients often leads to poor results, with fields consistent with the observations ruled out in history matching. We develop a method for adjusting for basis projection when history matching, so that an efficient and more accurate implausibility bound can be derived that is consistent with history matching using the computationally prohibitive spatial field.},
author = {Salter, J.},
file = {:C\:/Users/md624/OneDrive - University of Exeter/Research Papers/SalterJ.pdf:pdf},
pages = {1--329},
school = {University of Exeter},
title = {{Uncertainty quantification for spatial field data using expensive computer models: refocussed Bayesian calibration with optimal projection}},
year = {2017}
}
@phdthesis{Volodina2019,
abstract = {In this thesis, we provide the Uncertainty Quantification (UQ) tools to assist auto- matic and robust calibration of complex computer models. Our tools allow users to construct a cheap (statistical) surrogate, a Gaussian process (GP) emulator, based on a small number of climate model runs. History matching (HM), the calibration process of removing parameter space for which computer model outputs are incon- sistent with the observations, is combined with an emulator. The remaining subset of parameter space is termed the Not Ruled Out Yet (NROY). A weakly stationary GP with a covariance function that depends on the distance between two input points is the principal tool in UQ. However, the stationarity assumption is inadequate when we operate with a heterogeneous model response. In this thesis, we develop diagnostic-led nonstationary GP emulators with a kernel mixture. We employ diagnostics from a stationary GP fit to identify input regions with distinct model behaviour and obtain mixing functions for a kernel mixture. The result is a continuous emulator in parameter space that adapts to changes in model response behaviour. History matching has proven to be more effective when performed in waves. At each wave of HM, a new ensemble is obtained to update an emulator before finding an NROY space. In this thesis, we propose a Bayesian experimental design with a loss function that compares the volume of the NROY space obtained with an up- dated emulator to the volume of the âtrueâ NROY space obtained using a âperfectâ emulator. We combine Bayesian Design Criterion with our proposed nonstationary GP emulator to perform calibration of climate model.},
author = {Volodina, V.},
file = {:C\:/Users/md624/OneDrive - University of Exeter/Research Papers/VolodinaV.pdf:pdf},
pages = {1--327},
school = {University of Exeter},
title = {{Uncertainty Quantification for complex computer Bayesian optimal design for iterative refocussing}},
year = {2019}
}
@article{Xu2001,
author = {Xu, Lu and Zhang, Wen Jun},
doi = {10.1016/s0003-2670(01)01271-5},
file = {:C\:/Users/md624/OneDrive - University of Exeter/Research Papers/1-s2.0-S0003267001012715-main.pdf:pdf},
issn = {00032670},
journal = {Analytica Chimica Acta},
keywords = {Comparison of variable selection,Nitrobenzenes,QSAR/QSPR},
mendeley-groups = {Thesis/Chapter 3/Chapter 3 - AVs general,Thesis/Chapter 3/Chapter 3 - AVs general/Comparisons},
number = {1-2},
pages = {475--481},
title = {{Comparison of different methods for variable selection}},
volume = {446},
year = {2001}
}
@article{Heinze2018,
author = {Heinze, Georg and Wallisch, Christine and Dunkler, Daniela},
doi = {10.1002/bimj.201700067},
file = {:C\:/Users/md624/OneDrive - University of Exeter/Research Papers/BIMJ-60-431.pdf:pdf},
issn = {15214036},
journal = {Biometrical Journal},
keywords = {change-in-estimate criterion,penalized likelihood,resampling,statistical model,stepwise selection},
mendeley-groups = {Thesis/Chapter 3/Chapter 3 - AVs general,Thesis/Chapter 3/Chapter 3 - AVs general/Comparisons},
number = {3},
pages = {431--449},
pmid = {29292533},
title = {{Variable selection â A review and recommendations for the practicing statistician}},
volume = {60},
year = {2018}
}
@article{Hanke2024,
archivePrefix = {arXiv},
arxivId = {2302.12034},
author = {Hanke, Moritz and Dijkstra, Louis and Foraita, Ronja and Didelez, Vanessa},
doi = {10.1002/bimj.202200209},
eprint = {2302.12034},
file = {:C\:/Users/md624/OneDrive - University of Exeter/Research Papers/Biometrical J - 2023 - Hanke - Variable selection in linear regression models  Choosing the best subset is not always the.pdf:pdf},
issn = {15214036},
journal = {Biometrical Journal},
keywords = {Lasso,best subset selection,linear regression,mixed-integer optimization,variable selection},
number = {1},
pages = {1--20},
pmid = {37643390},
title = {{Variable selection in linear regression models: Choosing the best subset is not always the best choice}},
volume = {66},
year = {2024}
}
@article{Hastie2020,
author = {Hastie, Trevor and Tibshirani, Robert and Tibshirani, Ryan},
doi = {10.1214/19-STS733},
file = {:C\:/Users/md624/OneDrive - University of Exeter/Research Papers/19-STS733.pdf:pdf},
issn = {21688745},
journal = {Statistical Science},
keywords = {Regression,penalization.,selection},
mendeley-groups = {Thesis/Chapter 3/Chapter 3 - AVs general/Comparisons},
number = {4},
pages = {579--592},
title = {{Best Subset, Forward Stepwise or Lasso? Analysis and Recommendations Based on Extensive Comparisons}},
volume = {35},
year = {2020}
}
@ARTICLE{Akaike1974,
  author={Akaike, H.},
  journal={IEEE Transactions on Automatic Control}, 
  title={A new look at the statistical model identification}, 
  year={1974},
  volume={19},
  number={6},
  pages={716-723},
  keywords={Testing;Maximum likelihood estimation;Time series analysis;Estimation theory;Linear systems;Roundoff errors;History;Stochastic processes;Sampling methods;Art},
  doi={10.1109/TAC.1974.1100705}
}
@article{Schwarz1978,
author = {Schwarz, Gideon},
file = {:C\:/Users/md624/OneDrive - University of Exeter/Research Papers/1176344136 (1).pdf:pdf},
journal = {The Annals of Statistics},
keywords = {Akaike information criterion,Dimension,asymptotics},
number = {2},
pages = {461--464},
title = {{Estimating the Dimension of a Model}},
volume = {6},
year = {1978}
}
@article{Dunkler2014,
author = {Dunkler, Daniela and Plischke, Max and Leffondr{\'{e}}, Karen and Heinze, Georg},
doi = {10.1371/journal.pone.0113677},
file = {:C\:/Users/md624/OneDrive - University of Exeter/Research Papers/file.pdf:pdf},
issn = {19326203},
journal = {PLoS ONE},
mendeley-groups = {Thesis/Chapter 3/Chapter 3 - AVs general},
number = {11},
pages = {1--19},
pmid = {25415265},
title = {{Augmented backward elimination: A pragmatic and purposeful way to develop statistical models}},
volume = {9},
year = {2014}
}
@article{Gramacy2009,
  title={Adaptive design and analysis of supercomputer experiments},
  author={Gramacy, Robert B and Lee, Herbert KH},
  journal={Technometrics},
  volume={51},
  number={2},
  pages={130--145},
  year={2009},
  publisher={Taylor \& Francis}
}
@phdthesis{Moon2010,
  title={Design and analysis of computer experiments for screening input variables},
  author={Moon, Hyejung},
  year={2010},
  school={The Ohio State University}
}
@article{Moon2012,
  title={Two-stage sensitivity-based group screening in computer experiments},
  author={Moon, Hyejung and Dean, Angela M and Santner, Thomas J},
  journal={Technometrics},
  volume={54},
  number={4},
  pages={376--387},
  year={2012},
  publisher={Taylor \& Francis}
}
@incollection{Mackay1994,
  title={Automatic relevance determination for neural networks},
  author={MacKay, David JC and Neal, Radford M},
  booktitle={Technical Report in preparation.},
  year={1994},
  publisher={Cambridge University}
}
@article{Katoch2021,
author = {Katoch, Sourabh and Chauhan, Sumit Singh and Kumar, Vijay},
file = {:C\:/Users/md624/OneDrive - University of Exeter/Research Papers/s11042-020-10139-6.pdf:pdf},
isbn = {1104202010139},
journal = {Multimedia Tools and Applications},
keywords = {Cross,Genetic algorithm,Metaheuristic,Optimization,crossover,genetic algorithm,metaheuristic,mutation,optimization,selection},
mendeley-groups = {Thesis/Chapter 3/Chapter 3 - AVs general},
pages = {8091--8126},
publisher = {Multimedia Tools and Applications},
title = {{A review on genetic algorithm: past, present, and future}},
volume = {80},
year = {2021}
}
@article{Holland1992,
author = {Holland, John H.},
file = {:C\:/Users/md624/OneDrive - University of Exeter/Research Papers/Holland-GeneticAlgorithms-1992.pdf:pdf},
journal = {Scientific American},
mendeley-groups = {Thesis/Chapter 3},
number = {1},
pages = {66--73},
title = {{Genetic Algorithms}},
volume = {267},
year = {1992}
}
@online{Hoffmann2023,
title = {Understanding scipy.minimize part 1: The BFGS algorithm},
year = {2023},
organization = {Youtube},
author = {Folker Hoffmann},
url = {https://www.youtube.com/watch?v=VIoWzHlz7k8\&t=675s}
}
@article{Broyden1970,
author = {Broyden, C. G.},
doi = {10.1093/imamat/6.1.76},
file = {:C\:/Users/md624/OneDrive - University of Exeter/Research Papers/6-3-222.pdf:pdf},
issn = {02724960},
journal = {IMA Journal of Applied Mathematics (Institute of Mathematics and Its Applications)},
mendeley-groups = {Thesis/Chapter 3},
number = {1},
pages = {76--90},
title = {{The convergence of a class of double-rank minimization algorithms}},
volume = {6},
year = {1970}
}
@article{Fletcher1970,
author = {Fletcher, R},
file = {:C\:/Users/md624/OneDrive - University of Exeter/Research Papers/130317.pdf:pdf},
journal = {The Computer Journal},
mendeley-groups = {Thesis/Chapter 3},
number = {3},
pages = {317--322},
title = {{A new approach to variable metric algorithms}},
volume = {13},
year = {1970}
}
@article{Goldfarb1970,
author = {Goldfarb, Donald},
doi = {10.2307/2004873},
file = {:C\:/Users/md624/OneDrive - University of Exeter/Research Papers/S0025-5718-1970-0258249-6.pdf:pdf},
issn = {00255718},
journal = {Mathematics of Computation},
keywords = {and phrases,unconstrained},
mendeley-groups = {Thesis/Chapter 3},
number = {109},
pages = {23},
title = {{A Family of Variable-Metric Methods Derived by Variational Means}},
volume = {24},
year = {1970}
}
@article{Shanno1970,
abstract = {Quasi-Newton methods accelerate the steepest-descent technique for function minimization by using computational history to generate a sequence of approximations to the inverse of the Hessian matrix. This paper presents a class of approximating matrices as a function of a scalar parameter. The problem of optimal conditioning of these matrices under an appropriate norm as a function of the scalar parameter is investigated. A set of computational results verifies the superiority of the new methods arising from conditioning considerations to known methods.},
author = {Shanno, D. F.},
doi = {10.2307/2004840},
file = {:C\:/Users/md624/OneDrive - University of Exeter/Research Papers/S0025-5718-1970-0274029-X.pdf:pdf},
issn = {00255718},
journal = {Mathematics of Computation},
mendeley-groups = {Thesis/Chapter 3},
number = {111},
pages = {647},
title = {{Conditioning of Quasi-Newton Methods for Function Minimization}},
volume = {24},
year = {1970}
}
@article{Sun1996,
author = {Sun, Guo Wen and Shook, Thomas L. and Kay, Gregory L.},
doi = {10.1016/0895-4356(96)00025-X},
file = {:C\:/Users/md624/OneDrive - University of Exeter/Research Papers/1-s2.0-089543569600025X-main.pdf:pdf},
issn = {08954356},
journal = {Journal of Clinical Epidemiology},
keywords = {Bias,Bivariable analysis,Confounding,Multivariable analysis,Risk factors,Variable selection},
mendeley-groups = {Thesis/Chapter 3/Chapter 3 - AVs general},
number = {8},
pages = {907--916},
pmid = {8699212},
title = {{Inappropriate use of bivariable analysis to screen risk factors for use in multivariable analysis}},
volume = {49},
year = {1996}
}
@article{Heinze2017,
author = {Heinze, Georg and Dunkler, Daniela},
doi = {10.1111/tri.12895},
file = {:C\:/Users/md624/OneDrive - University of Exeter/Research Papers/Transplant International - 2016 - Heinze - Five myths about variable selection.pdf:pdf},
issn = {14322277},
journal = {Transplant International},
keywords = {association,explanatory models,multivariable modeling,prediction,statistical analysis},
mendeley-groups = {Thesis/Chapter 3/Chapter 3 - AVs general},
number = {1},
pages = {6--10},
pmid = {27896874},
title = {{Five myths about variable selection}},
volume = {30},
year = {2017}
}
@article{Tibshirani1996,
author = {Tibshirani, Robert},
file = {:C\:/Users/md624/OneDrive - University of Exeter/Research Papers/Tibshirani (JRSS-B 1996).pdf:pdf},
journal = {Journal of the Royal Statistical Society. Series B (Methodological)},
mendeley-groups = {Thesis/Chapter 3/Chapter 3 - AVs general},
number = {1},
pages = {267--288},
title = {{Regression Shrinkage and Selection via the Lasso}},
url = {jstor.org/stable/2346178},
volume = {58},
year = {1996}
}
@article{Zou2005,
author = {Zou, Hui and Hastie, Trevor},
doi = {10.1111/j.1467-9868.2005.00503.x},
file = {:C\:/Users/md624/OneDrive - University of Exeter/Research Papers/jrsssb_67_2_301.pdf:pdf},
issn = {13697412},
journal = {Journal of the Royal Statistical Society. Series B: Statistical Methodology},
keywords = {Grouping effect,LARS algorithm,Lasso,P â« n problem,Penalization,Variable selection},
mendeley-groups = {Thesis/Chapter 3/Chapter 3 - AVs general},
number = {2},
pages = {301--320},
title = {{Regularization and variable selection via the elastic net}},
volume = {67},
year = {2005}
}
@ARTICLE{Xu2012,
  author={Xu, Huan and Caramanis, Constantine and Mannor, Shie},
  journal={IEEE Transactions on Pattern Analysis and Machine Intelligence}, 
  title={Sparse Algorithms Are Not Stable: A No-Free-Lunch Theorem}, 
  year={2012},
  volume={34},
  number={1},
  pages={187-193},
  keywords={Signal processing algorithms;Stability criteria;Machine learning algorithms;Algorithm design and analysis;Support vector machines;Stability;sparsity;Lasso;regularization.},
  doi={10.1109/TPAMI.2011.177}}
@article{Furnival1974,
 ISSN = {00401706},
 URL = {http://www.jstor.org/stable/1267601},
 author = {George M. Furnival and Robert W. Wilson},
 journal = {Technometrics},
 number = {4},
 pages = {499--511},
 publisher = {[Taylor & Francis, Ltd., American Statistical Association, American Society for Quality]},
 title = {Regressions by Leaps and Bounds},
 urldate = {2024-05-04},
 volume = {16},
 year = {1974}
}
@article{Ratner2009,
author = {Ratner, Bruce},
doi = {10.1057/jt.2009.5},
file = {:C\:/Users/md624/OneDrive - University of Exeter/Research Papers/jt.2009.5.pdf:pdf},
issn = {09673237},
journal = {Journal of Targeting, Measurement and Analysis for Marketing},
mendeley-groups = {Thesis/Chapter 3/Chapter 3 - AVs general},
number = {2},
pages = {139--142},
title = {{The correlation coefficient: Its values range between +1/-1, or do they?}},
volume = {17},
year = {2009}
}
@article{Edwards2011,
author = {Edwards, Neil R. and Cameron, David and Rougier, Jonathan},
doi = {10.1007/s00382-010-0921-0},
file = {:C\:/Users/md624/OneDrive - University of Exeter/Research Papers/s00382-010-0921-0.pdf:pdf},
issn = {09307575},
journal = {Climate Dynamics},
keywords = {Intermediate complexity climate model,Probabilistic prediction,Thermohaline circulation,Uncertainty},
mendeley-groups = {Thesis/Chapter 3,Thesis/Chapter 3/Chapter 3 - AVs in HM},
number = {7-8},
pages = {1469--1482},
title = {{Precalibrating an intermediate complexity climate model}},
volume = {37},
year = {2011}
}
@Manual{hmer,
    title = {hmer: History Matching and Emulation Package},
    author = {Andrew Iskauskas and TJ McKinley},
    year = {2023},
    note = {R package version 1.5.6},
    url = {https://CRAN.R-project.org/package=hmer}
}
@article{Iskauskas2022,
archivePrefix = {arXiv},
arxivId = {2209.05265},
author = {Iskauskas, Andrew and Vernon, Ian and Goldstein, Michael and Scarponi, Danny and McKinley, Trevelyan J. and White, Richard G. and McCreesh, Nicky},
eprint = {2209.05265},
file = {:C\:/Users/md624/OneDrive - University of Exeter/Research Papers/2209.05265.pdf:pdf},
keywords = {calibration,emulation,history matching},
mendeley-groups = {Thesis/Chapter 3,Thesis/Chapter 3/Chapter 3 - AVs in HM},
pages = {1--40},
title = {{Emulation and History Matching using the hmer Package}},
url = {http://arxiv.org/abs/2209.05265},
year = {2022}
}
@article{Efroymson1960,
  title={Multiple regression analysis},
  author={Efroymson, Michael Alin},
  journal={Mathematical methods for digital computers},
  pages={191--203},
  year={1960},
  publisher={John Wiley \& Sons}
}
@article{Pearson1901,
abstract = {AbstractDownload full textRelated var addthis_config = { ui_cobrand: "Taylor & Francis Online", services_compact: "citeulike,netvibes,twitter,technorati,delicious,linkedin,facebook,stumbleupon,digg,google,more", pubid: "ra-4dff56cd6bb1830b" }; Add to shortlist Link Permalink http://dx.doi.org/10.1080/14786440109462720 Download Citation Recommend to: A friend},
author = {Pearson, Karl},
doi = {10.1080/14786440109462720},
file = {:C\:/Users/md624/OneDrive - University of Exeter/Research Papers/article.pdf:pdf},
issn = {1941-5982},
journal = {The London, Edinburgh, and Dublin Philosophical Magazine and Journal of Science},
number = {11},
pages = {559--572},
title = {{ LIII. On lines and planes of closest fit to systems of points in space }},
volume = {2},
year = {1901}
}
@article{Paananen2020,
archivePrefix = {arXiv},
arxivId = {1712.08048},
author = {Paananen, Topi and Piironen, Juho and Andersen, Michael Riis and Vehtari, Aki},
eprint = {1712.08048},
file = {:C\:/Users/md624/OneDrive - University of Exeter/Research Papers/paananen19a.pdf:pdf},
journal = {AISTATS 2019 - 22nd International Conference on Artificial Intelligence and Statistics},
keywords = {Automatic relevance determination},
title = {{Variable selection for Gaussian processes via sensitivity analysis of the posterior predictive distribution}},
volume = {89},
year = {2020}
}
@article{Saltelli2002,
    title = {Making best use of model evaluations to compute sensitivity indices},
    journal = {Computer Physics Communications},
    volume = {145},
    number = {2},
    pages = {280-297},
    year = {2002},
    issn = {0010-4655},
    doi = {https://doi.org/10.1016/S0010-4655(02)00280-1},
    url = {https://www.sciencedirect.com/science/article/pii/S0010465502002801},
    author = {Andrea Saltelli},
    keywords = {Sensitivity analysis, Sensitivity measures, Sensitivity indices, Importance measures}
}
@inbook{iooss_saltelli_2017,
    place={Switzerland},
    title={Introduction to Sensitivity Analysis},
    booktitle={Handbook of uncertainty quantification},
    publisher={Springer International Publishing AG},
    author={Iooss, Bertrand and Saltelli, Andrea},
    year={2017},
    pages={1103â1122}
 }
@phdthesis{Rosen1995,
author = {Rosen, Charles B.},
school = {STANFORD UNIVERSITY},
title = {{VISUALIZATION AND EXPLORATION OF HIGH-DIMENSIONAL FUNCTIONS USING THE FUNCTIONAL ANOVA DECOMPOSITION}},
year = {1995}
}
@article{mogptk,
    author = {T. {de Wolff} and A. {Cuevas} and F. {Tobar}},
    title = {{MOGPTK: The Multi-Output Gaussian Process Toolkit}},
    journal = "Neurocomputing",
    year = "2020",
    issn = "0925-2312",
    doi = "https://doi.org/10.1016/j.neucom.2020.09.085",
    url = "https://github.com/GAMES-UChile/mogptk"
}
@Manual{maximin,
    title = {maximin: Space-Filling Design under Maximin Distance},
    author = {Furong Sun and Robert B. Gramacy},
    year = {2021},
    note = {R package version 1.0-4},
    url = {https://CRAN.R-project.org/package=maximin},
  }
@Article{tidyverse,
    title = {Welcome to the {tidyverse}},
    author = {Hadley Wickham and Mara Averick and Jennifer Bryan and Winston Chang and Lucy D'Agostino McGowan and Romain FranÃ§ois and Garrett Grolemund and Alex Hayes and Lionel Henry and Jim Hester and Max Kuhn and Thomas Lin Pedersen and Evan Miller and Stephan Milton Bache and Kirill MÃ¼ller and Jeroen Ooms and David Robinson and Dana Paige Seidel and Vitalie Spinu and Kohske Takahashi and Davis Vaughan and Claus Wilke and Kara Woo and Hiroaki Yutani},
    year = {2019},
    journal = {Journal of Open Source Software},
    volume = {4},
    number = {43},
    pages = {1686},
    doi = {10.21105/joss.01686},
  }
@Book{MASS,
    title = {Modern Applied Statistics with S},
    author = {W. N. Venables and B. D. Ripley},
    publisher = {Springer},
    edition = {Fourth},
    address = {New York},
    year = {2002},
    note = {ISBN 0-387-95457-0},
    url = {https://www.stats.ox.ac.uk/pub/MASS4/},
  }
@Article{glmnet,
    title = {Elastic Net Regularization Paths for All Generalized Linear Models},
    author = {J. Kenneth Tay and Balasubramanian Narasimhan and Trevor Hastie},
    journal = {Journal of Statistical Software},
    year = {2023},
    volume = {106},
    number = {1},
    pages = {1--31},
    doi = {10.18637/jss.v106.i01},
  }
@Manual{leaps,
    title = {leaps: Regression Subset Selection},
    author = {Thomas Lumley and Alan Miller},
    year = {2020},
    note = {R package version 3.1},
    url = {https://CRAN.R-project.org/package=leaps},
  }
@Manual{genalg,
    title = {genalg: R Based Genetic Algorithm},
    author = {Egon Willighagen and Michel Ballings},
    year = {2022},
    note = {R package version 0.2.1},
    url = {https://CRAN.R-project.org/package=genalg},
  }
@Manual{reticulate,
    title = {reticulate: Interface to 'Python'},
    author = {Kevin Ushey and JJ Allaire and Yuan Tang},
    year = {2022},
    note = {R package version 1.24},
    url = {https://CRAN.R-project.org/package=reticulate},
  }
@Article{caret,
    title = {Building Predictive Models in R Using the caret Package},
    volume = {28},
    url = {https://www.jstatsoft.org/index.php/jss/article/view/v028i05},
    doi = {10.18637/jss.v028.i05},
    number = {5},
    journal = {Journal of Statistical Software},
    author = {{Kuhn} and {Max}},
    year = {2008},
    pages = {1â26},
  }
@Manual{faux,
    title = {faux: Simulation for Factorial Designs},
    author = {Lisa DeBruine},
    doi = {10.5281/zenodo.2669586},
    publisher = {Zenodo},
    year = {2023},
    note = {R package version 1.2.1},
    url = {https://debruine.github.io/faux/},
  }
@article{Bertsimas2016,
  title={Best subset selection via a modern optimization lens},
  author={Bertsimas, Dimitris and King, Angela and Mazumder, Rahul},
  year={2016}
}
@article{Darwin1859,
  title={On the origin of species: facsimile of the first edition},
  author={Darwin, Charles},
  year={1859},
  publisher={LONDON: JOHN MURRAY, ALBEMARLE STREET.}
}
@article{Obuchi2016,
archivePrefix = {arXiv},
arxivId = {1601.00881},
author = {Obuchi, Tomoyuki and Kabashima, Yoshiyuki},
doi = {10.1088/1742-5468/2016/05/053304},
eprint = {1601.00881},
file = {:C\:/Users/md624/OneDrive - University of Exeter/Research Papers/Obuchi_2016_J._Stat._Mech._2016_053304.pdf:pdf},
issn = {17425468},
journal = {Journal of Statistical Mechanics: Theory and Experiment},
keywords = {Learning theory,Message-passing algorithms,Statistical inference},
number = {5},
pages = {0--36},
publisher = {IOP Publishing},
title = {{Cross validation in LASSO and its acceleration}},
volume = {2016},
year = {2016}
}
@article{Owen1992,
  title={Orthogonal arrays for computer experiments, integration and visualization},
  author={Owen, Art B},
  journal={Statistica Sinica},
  pages={439--452},
  year={1992},
  publisher={JSTOR}
}
@article{Efron1981,
  title={The jackknife estimate of variance},
  author={Efron, Bradley and Stein, Charles},
  journal={The Annals of Statistics},
  pages={586--596},
  year={1981},
  publisher={JSTOR}
}
@article{Bastos2009,
abstract = {Mathematical models, usually implemented in computer programs known as simulators, are widely used in all areas of science and technology to represent complex real-world phenomena. Simulators are often so complex that they take appreciable amounts of computer time or other resources to run. In this context, a methodology has been developed based on building a statistical representation of the simulator, known as an emulator. The principal approach to building emulators uses Gaussian processes. This work presents some diagnostics to validate and assess the adequacy of a Gaussian process emulator as surrogate for the simulator. These diagnostics are based on comparisons between simulator outputs and Gaussian process emulator outputs for some test data, known as validation data, defined by a sample of simulator runs not used to build the emulator. Our diagnostics take care to account for correlation between the validation data. To illustrate a validation procedure, we apply these diagnostics to two different data sets. {\textcopyright} 2009 American Statistical Association.},
author = {Bastos, Leonardo S. and O'Hagan, Anthony},
doi = {10.1198/TECH.2009.08019},
file = {:C\:/Users/md624/OneDrive - University of Exeter/Research Papers/Diagnostics for Gaussian Process Emulators.pdf:pdf},
issn = {00401706},
journal = {Technometrics},
keywords = {Bayesian inference,Computer experiments,Diagnostics,Emulation,Gaussian process},
number = {4},
pages = {425--438},
title = {{Diagnostics for gaussian process emulators}},
url = {https://doi.org/10.1198/TECH.2009.08019},
volume = {51},
year = {2009}
}
@article{Farrow1993,
abstract = {SUMMARY: This paper offers a study in the application of Bayes linear methods. We outline a general approach to the analysis of mean effects for grouped multivariate repeated measurement studies, based upon partial belief specification. We suggest a method for coherent partial prior specification for such structures, based on moment evaluations for exchangeable data. We describe the general collection of interpretive and diagnostic tools termed 'Bayes linear methods', and suggest a simple criterion for trial design. The theory is illustrated by analysis of a crossover trial concerned with side effects of kidney dialysis. {\textcopyright} 1993 Biometrika Trust.},
author = {Farrow, Malcolm and Goldstein, Michael},
doi = {10.1093/biomet/80.1.39},
file = {:C\:/Users/md624/OneDrive - University of Exeter/Research Papers/80-1-39.pdf:pdf},
issn = {00063444},
journal = {Biometrika},
keywords = {Bayes diagnostics,Bayes linear methods,Belief transform,Crossover trial,Exchangeability,Optimal design},
number = {1},
pages = {39--59},
title = {{Bayes linear methods for grouped multivariate repeated measurement studies with application to crossover trials}},
volume = {80},
year = {1993}
}
@article{Craig1996,
abstract = {INCLUDED},
author = {Craig, P S and Goldstein, M and Seheult, A H and Smith, J A},
doi = {10.1093/oso/9780198523567.003.0004},
journal = {Bayesian Statistics 5},
mendeley-groups = {Thesis/Chapter 3/Chapter 3 - AVs in HM},
pages = {69--96},
title = {{Bayes Linear Strategies for Matching Hydrocarbon Reservoir History - ON MY DESK}},
year = {1996}
}
@article{Sobol1993,
  title={Sensitivity estimates for nonlinear mathematical models},
  author={Sobo{\'l}, IM},
  journal={Math. Model. Comput. Exp.},
  volume={1},
  pages={407},
  year={1993}
}
@article{Pukelsheim1994,
  title={The three sigma rule},
  author={Pukelsheim, Friedrich},
  journal={The American Statistician},
  volume={48},
  number={2},
  pages={88--91},
  year={1994},
  publisher={Taylor \& Francis}
}
@article{Salter2022,
  title={Efficient calibration for high-dimensional computer model output using basis methods},
  author={Salter, James M and Williamson, Daniel B},
  journal={International Journal for Uncertainty Quantification},
  volume={12},
  number={6},
  year={2022},
  publisher={Begel House Inc.}
}
@article{Paun2024,
author = {Paun, L. Mihaela and Colebank, Mitchel J. and Taylor-LaPole, Alyssa and Olufsen, Mette S. and Ryan, William and Murray, Iain and Salter, James M. and Applebaum, Victor and Dunne, Michael and Hollins, Jake and Kimpton, Louise and Volodina, Victoria and Xiong, Xiaoyu and Husmeier, Dirk},
doi = {10.1016/j.cma.2024.117193},
file = {:C\:/Users/md624/OneDrive - University of Exeter/Research Papers/SECRET_paper.pdf:pdf},
issn = {00457825},
journal = {Computer Methods in Applied Mechanics and Engineering},
keywords = {Computational fluid-dynamics,Parameter inference,Personalised healthcare,Statistical emulation,Uncertainty quantification},
number = {May},
pages = {117193},
publisher = {Elsevier B.V.},
title = {{SECRET: Statistical Emulation for Computational Reverse Engineering and Translation with applications in healthcare}},
url = {https://doi.org/10.1016/j.cma.2024.117193},
volume = {430},
year = {2024}
}
@article{Salter2019,
archivePrefix = {arXiv},
arxivId = {1801.08184},
author = {Salter, James M. and Williamson, Daniel B. and Scinocca, John and Kharin, Viatcheslav},
doi = {10.1080/01621459.2018.1514306},
eprint = {1801.08184},
file = {:C\:/Users/md624/OneDrive - University of Exeter/Research Papers/01621459.2018.pdf:pdf},
issn = {1537274X},
journal = {Journal of the American Statistical Association},
keywords = {Bayesian calibration,Climate models,History matching,Rotation,Tuning},
number = {528},
pages = {1800--1814},
publisher = {Taylor & Francis},
title = {{Uncertainty Quantification for Computer Models With Spatial Output Using Calibration-Optimal Bases}},
url = {https://doi.org/10.1080/01621459.2018.1514306},
volume = {114},
year = {2019}
}
@article{Morris1995,
abstract = {Recent work by Johnson et al. (J. Statist. Plann. Inference 26 (1990) 131-148) establishes equivalence of the maximin distance design criterion and an entropy criterion motivated by function prediction in a Bayesian setting. The latter criterion has been used by Currin et al. (J. Amer. Statist. Assoc. 86 (1991) 953-963) to design experiments for which the motivating application is approximation of a complex deterministic computer model. Because computer experiments often have a large number of controlled variables (inputs), maximin designs of moderate size are often concentrated in the corners of the cuboidal design region, i.e. each input is represented at only two levels. Here we will examine some maximin distance designs constructed within the class of Latin hypercube arrangements. The goal of this is to find designs which offer a compromise between the entropy/maximin criterion, and good projective properties in each dimension (as guaranteed by Latin hypercubes). A simulated annealing search algorithm is presented for constructing these designs, and patterns apparent in the optimal designs are discussed. {\textcopyright} 1995.},
author = {Morris, Max D. and Mitchell, Toby J.},
doi = {10.1016/0378-3758(94)00035-T},
file = {:C\:/Users/md624/OneDrive - University of Exeter/Research Papers/1-s2.0-037837589400035T-main.pdf:pdf},
issn = {03783758},
journal = {Journal of Statistical Planning and Inference},
keywords = {Bayesian prediction,Computer experiment,Computer model,Interpolation,Latin hypercube design,Maximin design,Random functions},
number = {3},
pages = {381--402},
title = {{Exploratory designs for computational experiments}},
volume = {43},
year = {1995}
}
@article{Giordano2020,
abstract = {In Italy, 128,948 confirmed cases and 15,887 deaths of peo- ple who tested positive for SARS-CoV-2 were registered as of 5 April 2020. Ending the global SARS-CoV-2 pandemic requires implementation of multiple population-wide strate- gies, including social distancing, testing and contact tracing. We propose a new model that predicts the course of the epi- demic to help plan an effective control strategy. The model considers eight stages of infection: susceptible (S), infected (I), diagnosed (D), ailing (A), recognized (R), threatened (T), healed (H) and extinct (E), collectively termed SIDARTHE. Our SIDARTHE model discriminates between infected indi- viduals depending on whether they have been diagnosed and on the severity of their symptoms. The distinction between diagnosed and non-diagnosed individuals is important because the former are typically isolated and hence less likely to spread the infection. This delineation also helps to explain misperceptions of the case fatality rate and of the epidemic spread. We compare simulation results with real data on the COVID-19 epidemic in Italy, and we model possible scenarios of implementation of countermeasures. Our results demon- strate that restrictive social-distancing measures will need to be combined with widespread testing and contact tracing to end the ongoing COVID-19 pandemic.},
author = {Giordano, Giulia and Blanchini, Franco and Bruno, Raffaele and Colaneri, Patrizio and Filippo, Alessandro Di and Matteo, Angela Di and Colaneri, Marta},
file = {:C\:/Users/md624/OneDrive - University of Exeter/Research Papers/s41591-020-0883-7.pdf:pdf},
isbn = {4159102008837},
journal = {Nature Medicine},
number = {6},
pages = {855--860},
title = {{Modelling the COVID-19 epidemic and implementation of population-wide interventions in Italy}},
volume = {26},
year = {2020}
}
@article{Chauhan2014,
abstract = {This paper aims to study a SIR model with and without vaccination. A reproduction number R 0 is defined and it is obtained that the disease-free equilibrium point is unstable if {\'{i}} Âµ{\'{i}}Â±Â{\'{i}} Âµ{\'{i}}Â±Â 0 > 1 and the non-trivial endemic equilibrium point exist if {\'{i}} Âµ{\'{i}}Â±Â{\'{i}} Âµ{\'{i}}Â±Â 0 > 1 in the absence of vaccination. Further, a new reproduction number {\'{i}} Âµ{\'{i}}Â±Â{\'{i}} Âµ{\'{i}}Â±Â {\'{i}} Âµ{\'{i}}Â±{\pounds}{\'{i}} Âµ{\'{i}}Â±{\pounds} is defined for the model in which vaccination is introduced. The linear stability and the global stability of both the models are discussed and the comparison of both the models is done regarding the existence of the disease-free equilibrium point and endemic equilibrium point. Finally, a numerical example is given in support of the result.},
author = {Chauhan, Sudipa and Misra, Om Prakash and Dhar, Joydip},
doi = {10.5923/j.ajcam.20140401.03},
file = {:C\:/Users/md624/OneDrive - University of Exeter/Research Papers/10.5923.j.ajcam.20140401.03.pdf:pdf},
journal = {American Journal of Computational and Applied Mathematics},
keywords = {Reproduction number,SIR model,Stability,Vaccination},
number = {1},
pages = {17--23},
title = {{Stability Analysis of Sir Model with Vaccination}},
volume = {4},
year = {2014}
}
@book{beckley2013,
place={Tennessee},
edition={1},
title={Modeling epidemics with differential equations},
url={https://www.tnstate.edu/mathematics/mathreu/filesreu/GroupProjectSIR.pdf},
publisher={Tennessee State University},
author={Beckley, Ross and Weatherspoon, Cametria and Alexander, Michael and Chandler, Marissa and Johnson, Anthony and Bhatt, Ghan S},
year={2013}
}
@article{swallow21,
author = {Ben Swallow and Paul Birrell and Joshua Blake and Mark Burgman and Peter Challenor and Luc E.
Coffeng and Philip Dawid and Daniela {De Angelis} and Michael Goldstein and Victoria Hemming and Glenn Marion and Trevelyan J. McKinley and Christopher Overton and Jasmina Panovska-Griffiths and Lorenzo Pellis and Will Probert and Katriona Shea and Daniel Villela and Ian Vernon},
journal = {\emph{Preprint available at} https://www.newton.ac.uk/documents/preprints/},
title = {{Challenges in estimation, uncertainty quantification and elicitation for pandemic modelling}},
year = {2021}
}
@misc{porphyre2020,
title={Scottish COVID Response Consortium (SCRC): EERA model overview},
author={Porphyre, Thibaud and Bronsvoort, Mark and Fox, Peter and Zarebski, Kristian and Xia, Qingfeng and Gadgil, Sanket},
year={2020},
url = {https://github.com/ScottishCovidResponse/Covid19\_EERAModel}
}
@article{Challenor2013,
abstract = {Metamodels (or emulators) are statistical tools for the analysis of large complex simulation models. They consist of a Gaussian (or second order) process (kriging) fitted to a designed set of simulator runs. Once an emulator has been built, it is important that it is validated against some independent runs of the simulator. This paper considers the design of experiments for the validation of the fitted metamodel. All the proposed designs are based on maximin Latin hypercubes and add an extra criterion to be optimised based on the distances between the points in the validation and original designs. Simulation experiments are carried out to determine how well each design performs against the alternative criteria. {\textcopyright} 2013 Operational Research Society Ltd. All rights reserved.},
author = {Challenor, P.},
doi = {10.1057/jos.2013.17},
file = {:C\:/Users/md624/OneDrive - University of Exeter/Research Papers/challenor2013.pdf:pdf},
issn = {17477786},
journal = {Journal of Simulation},
keywords = {computer experiment,design,emulator,kriging,metamodel,validation},
number = {4},
pages = {290--296},
title = {{Experimental design for the validation of kriging metamodels in computer experiments}},
volume = {7},
year = {2013}
}
@article{chen2020,
      title={{RAMPVIS: Towards a New Methodology for Developing Visualisation Capabilities for Large-scale Emergency Responses}}, 
      author={M. Chen and A. Abdul-Rahman and D. Archambault and J. Dykes and A. Slingsby and P. D. Ritsos and T. Torsney-Weir and C. Turkay and B. Bach and A. Brett and H. Fang and R. Jianu and S. Khan and R. S. Laramee and P. H. Nguyen and R. Reeve and J. C. Roberts and F. Vidal and Q. Wang and J. Wood and K. Xu},
      year={2020},
      journal={arXiv:2012.04757},
      archivePrefix={arXiv},
      primaryClass={cs.HC}
}
@inbook{storch_zwiers_1999,
 place={Cambridge},
 title={Empirical Orthogonal Functions},
 booktitle={Statistical Analysis in climate research},
 publisher={Cambridge University Press},
 author={Storch, H. Von and Zwiers, F. W.},
 year={1999},
 pages={293â316}
}
@inbook{greenacre_1984, 
    place={London}, 
    title={Appendix A}, 
    booktitle={Theory and applications of correspondence analysis}, 
    publisher={Academic Press}, 
    author={Greenacre, Michael J.}, 
    year={1984}, 
    pages={340â351}
}
@article{Williamson2013,
abstract = {In this paper we tackle the problem of generating uniform designs in very small subregions of computer model input space that have been identified in previous experiments as worthy of further study. The method is capable of producing uniform designs in subregions of computer model input space defined by a membership function that consists of a continuous function passing a threshold test, and does so far more efficiently than current methods when these subregions are small. Our application is designing for regions of input space that are not ruled out by history matching, a statistical methodology applied in numerous diverse scientific applications whereby model runs are used to cut out regions of input space that are incompatible with real world observations. History matching defines a membership function for a region of input space that is not ruled out yet by observations in the form of a distance metric called implausibility. We use this distance metric to drive a new type of Evolutionary Monte Carlo algorithm with a uniform distribution on the not ruled out yet region as its target distribution. The algorithm can locate and generate uniform points within extremely small subspaces of the computer model input space with complex and even disconnected topologies. We illustrate the performance of the technique in comparison to current methods with a number of idealised examples. We then apply our algorithm to generating an optimal design for the not ruled out yet region of a galaxy simulation model called GALFORM following 4 previous waves of history matching where the target region is 0.001% the volume of the input space.},
archivePrefix = {arXiv},
arxivId = {1309.3520},
author = {Williamson, Daniel and Vernon, Ian},
eprint = {1309.3520},
file = {:C\:/Users/md624/OneDrive - University of Exeter/Research Papers/williamson2018.pdf:pdf},
keywords = {computer models,emulation,history matching,implausibility driven evolutionary monte carlo,input space},
pages = {1--31},
title = {{Efficient uniform designs for multi-wave computer experiments}},
url = {http://arxiv.org/abs/1309.3520},
year = {2013}
}
@article{Kloek1978,
    title={Bayesian estimates of equation system parameters: an application of integration by Monte Carlo},
    author={Kloek, Teun and Van Dijk, Herman K},
    journal={Econometrica: Journal of the Econometric Society},
    pages={1--19},
    year={1978},
    publisher={JSTOR}
}
@article{Conti2009,
abstract = {Computer models are widely used in scientific research to study and predict the behaviour of complex systems. The run times of computer-intensive simulators are often such that it is impractical to make the thousands of model runs that are conventionally required for sensitivity analysis, uncertainty analysis or calibration. In response to this problem, highly efficient techniques have recently been developed based on a statistical meta-model (the emulator) that is built to approximate the computer model. The approach, however, is less straightforward for dynamic simulators, designed to represent time-evolving systems. Generalisations of the established methodology to allow for dynamic emulation are here proposed and contrasted. Advantages and difficulties are discussed and illustrated with an application to the Sheffield Dynamic Global Vegetation Model, developed within the UK Centre for Terrestrial Carbon Dynamics.},
author = {Conti, Stefano and O'Hagan, Anthony},
doi = {10.1016/j.jspi.2009.08.006},
file = {:C\:/Users/md624/OneDrive - University of Exeter/Research Papers/1-s2.0-S0378375809002559-main.pdf:pdf},
issn = {03783758},
journal = {Journal of Statistical Planning and Inference},
keywords = {Bayesian inference,Computer experiments,Dynamic models,Hierarchical models},
number = {3},
pages = {640--651},
title = {{Bayesian emulation of complex multi-output and dynamic computer models}},
volume = {140},
year = {2009}
}
@article{Wilkinson_2010,
title={Bayesian calibration of expensive multivariate computer experiments},
DOI={10.1002/9780470685853.ch10},
journal={LargeâScale Inverse Problems and Quantification of Uncertainty},
author={Wilkinson, R. D.},
year={2010},
month={Oct},
pages={195â215}
}
@article{Sexton2012,
  title={Multivariate probabilistic projections using imperfect climate models part I: outline of methodology},
  author={Sexton, David MH and Murphy, James M and Collins, Mat and Webb, Mark J},
  journal={Climate dynamics},
  volume={38},
  pages={2513--2542},
  year={2012},
  publisher={Springer}
}
@article{Liu2009,
author = {Liu, Fei and West, Mike},
doi = {10.1214/09-BA415},
file = {:C\:/Users/md624/OneDrive - University of Exeter/Research Papers/09-BA415.pdf:pdf},
issn = {19360975},
journal = {Bayesian Analysis},
keywords = {Backward sampling,Computer model emulation,Dynamic linear model,Forwarding filtering,Gaussian process,Markov chain Monte Carlo,Time,Varying autoregression},
number = {2},
pages = {393--412},
title = {{A dynamic modelling strategy for bayesian computer model emulation}},
volume = {4},
year = {2009}
}
@article{Lee2013,
author = {Lee, L. A. and Pringle, K. J. and Reddington, C. L. and Mann, G. W. and Stier, P. and Spracklen, D. V. and Pierce, J. R. and Carslaw, K. S.},
doi = {10.5194/acp-13-8879-2013},
file = {:C\:/Users/md624/OneDrive - University of Exeter/Research Papers/acp-13-8879-2013.pdf:pdf},
issn = {16807316},
journal = {Atmospheric Chemistry and Physics},
number = {17},
pages = {8879--8914},
title = {{The magnitude and causes of uncertainty in global model simulations of cloud condensation nuclei}},
volume = {13},
year = {2013}
}
@article{Kermack1927,
author = {Kermack, William Ogilvy  and McKendrick, A. G.  and Walker, Gilbert Thomas },
title = {A contribution to the mathematical theory of epidemics},
journal = {Proceedings of the Royal Society of London. Series A, Containing Papers of a Mathematical and Physical Character},
volume = {115},
number = {772},
pages = {700-721},
year = {1927},
doi = {10.1098/rspa.1927.0118},
URL = {https://royalsocietypublishing.org/doi/abs/10.1098/rspa.1927.0118},
eprint = {https://royalsocietypublishing.org/doi/pdf/10.1098/rspa.1927.0118}
}
@article{Neal1997b,
  title={Markov chain Monte Carlo methods based onslicing'the density function},
  author={Neal, Radford M},
  journal={Preprint},
  year={1997},
  publisher={Citeseer}
}
@article{Wong2015,
  title={Performance evaluation of classification algorithms by k-fold and leave-one-out cross validation},
  author={Wong, Tzu-Tsung},
  journal={Pattern recognition},
  volume={48},
  number={9},
  pages={2839--2846},
  year={2015},
  publisher={Elsevier}
}
@article{Zhang2018,
  title={Efficient history matching with dimensionality reduction methods for reservoir simulations},
  author={Zhang, Dongmei and Shen, Ao and Jiang, Xinwei and Kang, Zhijiang},
  journal={Simulation},
  volume={94},
  number={8},
  pages={739--751},
  year={2018},
  publisher={SAGE Publications Sage UK: London, England}
}
@article{Rougier2009,
  title={Analyzing the climate sensitivity of the HadSM3 climate model using ensembles from different but related experiments},
  author={Rougier, Jonathan and Sexton, David MH and Murphy, James M and Stainforth, David},
  journal={Journal of Climate},
  volume={22},
  number={13},
  pages={3540--3557},
  year={2009},
  publisher={American Meteorological Society}
}
@book{Hilt1977,
  title={Ridge, a computer program for calculating ridge regression estimates},
  author={Hilt, Donald E and Seegrist, Donald W},
  year={1977},
  publisher={Department of Agriculture, Forest Service, Northeastern Forest Experiment~â¦}
}
@misc{VanWieringen2023,
      title={Lecture notes on ridge regression}, 
      author={Wessel N. van Wieringen},
      year={2023},
      eprint={1509.09169},
      archivePrefix={arXiv},
      primaryClass={stat.ME},
      url={https://arxiv.org/abs/1509.09169}, 
}
@article{Loeppky2009,
abstract = {We provide reasons and evidence supporting the informal rule that the number of runs for an effective initial computer experiment should be about 10 times the input dimension. Our arguments quantify two key characteristics of computer codes that affect the sample size required for a desired level of accuracy when approximating the code via a Gaussian process (GP). The first characteristic is the total sensitivity of a code output variable to all input variables; the second corresponds to the way this total sensitivity is distributed across the input variables, specifically the possible presence of a few prominent input factors and many impotent ones (i.e., effect sparsity). Both measures relate directly to the correlation structure in the GP approximation of the code. In this way, the article moves toward a more formal treatment of sample size for a computer experiment. The evidence supporting these arguments stems primarily from a simulation study and via specific codes modeling climate and ligand activation of G-protein. {\textcopyright} 2009 American Statistical Association.},
author = {Loeppky, Jason L. and Sacks, Jerome and Welch, William J.},
doi = {10.1198/TECH.2009.08040},
file = {:C\:/Users/md624/OneDrive - University of Exeter/Research Papers/loeppky2009.pdf:pdf},
issn = {00401706},
journal = {Technometrics},
keywords = {Curse of dimensionality,Effect sparsity,Gaussian process,Latin hypercube design,Prediction accuracy,Random function},
number = {4},
pages = {366--376},
title = {{Choosing the sample size of a computer experiment: A practical guide}},
volume = {51},
year = {2009}
}
@article{Csiszar1975,
  title={I-divergence geometry of probability distributions and minimization problems},
  author={Csisz{\'a}r, Imre},
  journal={The annals of probability},
  pages={146--158},
  year={1975},
  publisher={JSTOR}
}
@article{Bhattacharyya1943,
  title={On a measure of divergence between two statistical populations defined by their probability distribution},
  author={Bhattacharyya, Anil},
  journal={Bulletin of the Calcutta Mathematical Society},
  volume={35},
  pages={99--110},
  year={1943}
}
@article{Randic1991,
  title={Orthogonal molecular descriptors},
  author={Randic, Milan},
  journal={Nouveau journal de chimie (1977)},
  volume={15},
  number={7},
  pages={517--525},
  year={1991}
}
@article{Soskic1996,
 author = {Å oÅ¡kiÄ, Milan and PlavÅ¡iÄ, Dejan and TrinajstiÄ, Nenad},
 title = {Link between Orthogonal and Standard Multiple Linear Regression Models},
 journal = {Journal of Chemical Information and Computer Sciences},
 volume = {36},
 number = {4},
 pages = {829-832},
 year = {1996},
 doi = {10.1021/ci950183m},
 URL = {https://doi.org/10.1021/ci950183m},
 eprint = {https://doi.org/10.1021/ci950183m}
}
@article{Joliffe1992,
author = {I Joliffe and Bjt Morgan},
title ={Principal component analysis and exploratory factor analysis},
journal = {Statistical Methods in Medical Research},
volume = {1},
number = {1},
pages = {69-95},
year = {1992},
doi = {10.1177/096228029200100105},
note ={PMID: 1341653},
URL = {
    https://doi.org/10.1177/096228029200100105
},
eprint = { https://doi.org/10.1177/096228029200100105
}
}
@article{Borovitskiy2020,
  title={Mat{\'e}rn Gaussian processes on Riemannian manifolds},
  author={Borovitskiy, Viacheslav and Terenin, Alexander and Mostowsky, Peter and others},
  journal={Advances in Neural Information Processing Systems},
  volume={33},
  pages={12426--12437},
  year={2020}
}
@article{Metropolis1953,
    author = {Metropolis, Nicholas and Rosenbluth, Arianna W. and Rosenbluth, Marshall N. and Teller, Augusta H. and Teller, Edward},
    title = {Equation of State Calculations by Fast Computing Machines},
    journal = {The Journal of Chemical Physics},
    volume = {21},
    number = {6},
    pages = {1087-1092},
    year = {1953},
    month = {06},
    abstract = {A general method, suitable for fast computing machines, for investigating such properties as equations of state for substances consisting of interacting individual molecules is described. The method consists of a modified Monte Carlo integration over configuration space. Results for the twoâdimensional rigidâsphere system have been obtained on the Los Alamos MANIAC and are presented here. These results are compared to the free volume equation of state and to a fourâterm virial coefficient expansion.},
    issn = {0021-9606},
    doi = {10.1063/1.1699114},
    url = {https://doi.org/10.1063/1.1699114},
    eprint = {https://pubs.aip.org/aip/jcp/article-pdf/21/6/1087/18802390/1087\_1\_online.pdf},
}
@article{nimble, 
  author = {{de Valpine}, P. and Turek, D. and Paciorek, C.J. and Anderson-Bergman, C. and {Temple Lang}, D. and Bodik, R.},
  title = {Programming with models: writing statistical algorithms for general model structures with {NIMBLE}},
  year = {2017}, 
  journal = {Journal of Computational and Graphical Statistics},
  volume = 26,
  pages = {403-417},
  doi = {10.1080/10618600.2016.1172487}
}
@Misc{stan,
    title = {{RStan}: the {R} interface to {Stan}},
    author = {{Stan Development Team}},
    note = {R package version 2.32.6},
    year = {2024},
    url = {https://mc-stan.org/},
  }
@article{Aghajanyan2020,
  author       = {Armen Aghajanyan and
                  Luke Zettlemoyer and
                  Sonal Gupta},
  title        = {Intrinsic Dimensionality Explains the Effectiveness of Language Model
                  Fine-Tuning},
  journal      = {CoRR},
  volume       = {abs/2012.13255},
  year         = {2020},
  url          = {https://arxiv.org/abs/2012.13255},
  eprinttype    = {arXiv},
  eprint       = {2012.13255},
  timestamp    = {Tue, 05 Jan 2021 16:02:31 +0100},
  biburl       = {https://dblp.org/rec/journals/corr/abs-2012-13255.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}
@INPROCEEDINGS{BuiThanh2012,
  author={Bui-Thanh, Tan and Burstedde, Carsten and Ghattas, Omar and Martin, James and Stadler, Georg and Wilcox, Lucas C.},
  booktitle={SC '12: Proceedings of the International Conference on High Performance Computing, Networking, Storage and Analysis}, 
  title={Extreme-scale UQ for Bayesian inverse problems governed by PDEs}, 
  year={2012},
  volume={},
  number={},
  pages={1-11},
  keywords={Inverse problems;Approximation methods;Covariance matrix;Uncertainty;Vectors;Bayesian methods;Eigenvalues and eigenfunctions},
  doi={10.1109/SC.2012.56}}

@article{Saghafi2024,
abstract = {Alzheimer's disease (AD) is believed to occur when abnormal amounts of the proteins amyloid beta and tau aggregate in the brain, resulting in a progressive loss of neuronal function. Hippocampal neurons in transgenic mice with amyloidopathy or tauopathy exhibit altered intrinsic excitability properties. We used deep hybrid modeling (DeepHM), a recently developed parameter inference technique that combines deep learning with biophysical modeling, to map experimental data recorded from hippocampal CA1 neurons in transgenic AD mice and age-matched wildtype littermate controls to the parameter space of a conductance-based CA1 model. Although mechanistic modeling and machine learning methods are by themselves powerful tools for approximating biological systems and making accurate predictions from data, when used in isolation these approaches suffer from distinct shortcomings: model and parameter uncertainty limit mechanistic modeling, whereas machine learning methods disregard the underlying biophysical mechanisms. DeepHM addresses these shortcomings by using conditional generative adversarial networks to provide an inverse mapping of data to mechanistic models that identifies the distributions of mechanistic modeling parameters coherent to the data. Here, we demonstrated that DeepHM accurately infers parameter distributions of the conductance-based model on several test cases using synthetic data generated with complex underlying parameter structures. We then used DeepHM to estimate parameter distributions corresponding to the experimental data and infer which ion channels are altered in the Alzheimer's mouse models compared to their wildtype controls at 12 and 24 months. We found that the conductances most disrupted by tauopathy, amyloidopathy, and aging are delayed rectifier potassium, transient sodium, and hyperpolarization-activated potassium, respectively.},
author = {Saghafi, Soheil and Rumbell, Timothy and Gurev, Viatcheslav and Kozloski, James and Tamagnini, Francesco and Wedgwood, Kyle C.A. and Diekman, Casey O.},
doi = {10.1007/s11538-024-01273-5},
file = {:C\:/Users/md624/University of Exeter/Wedgwood, Kyle - Cross model analysis/Notes/SaghafiPaper.pdf:pdf},
issn = {15229602},
journal = {Bulletin of Mathematical Biology},
keywords = {Generative adversarial network,Parameter inference,Population of models,Pyramidal neuron excitability},
mendeley-groups = {Post-doc},
number = {5},
pages = {1--29},
pmid = {38528167},
publisher = {Springer US},
title = {{Inferring Parameters of Pyramidal Neuron Excitability in Mouse Models of Alzheimer's Disease Using Biophysical Modeling and Deep Learning}},
url = {https://doi.org/10.1007/s11538-024-01273-5},
volume = {86},
year = {2024}
}
